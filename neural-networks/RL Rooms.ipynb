{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "based off of article: \n",
    "http://mnemstudio.org/path-finding-q-learning-tutorial.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#create rooms\n",
    "adj_mat = {\n",
    "    0: [4],\n",
    "    1: [3,5],\n",
    "    2: [3],\n",
    "    3: [1,2,4],\n",
    "    4: [0,3,5],\n",
    "    5: [1,4,5]\n",
    "}\n",
    "\n",
    "#rewards. 100 points for reaching room 5. -1 denotes\n",
    "#pairs without routes\n",
    "rewards = np.array([[-1,-1,-1,-1,0,-1],\n",
    "                    [-1,-1,-1,0,-1,100],\n",
    "                    [-1,-1,-1,0,-1,-1],\n",
    "                    [-1,0,0,-1,0,-1],\n",
    "                    [0,-1,-1,0,-1,100],\n",
    "                    [-1,0,-1,-1,0,100]])\n",
    "\n",
    "#random room rate, to influence the room selection to the best learned so far\n",
    "lr = 0.7\n",
    "\n",
    "#gamma influences how much impact future reward have in comparison to \n",
    "#past/current reward.\n",
    "gamma = 0.8\n",
    "\n",
    "#update q function\n",
    "def update_q(state, action, debug=False):\n",
    "    #get list of actions that can be done from next state\n",
    "    next_actions = adj_mat[action]\n",
    "\n",
    "    #calculate q-table\n",
    "    q_mat[state, action] = rewards[state, action] +\\\n",
    "                           gamma * np.max([q_mat[action, i] \n",
    "                                          for i in next_actions])\n",
    "    if debug:\n",
    "        print q_mat\n",
    "        \n",
    "def find_best_action(state):\n",
    "    #get best action value for this state. np.argmax only returns the first occurence\n",
    "    #so we have a different approach\n",
    "    #action = q_mat[state].argmax(axis=0)\n",
    "    \n",
    "    #get indices of maximum values\n",
    "    max_val = np.max(q_mat[state])\n",
    "    max_indices = np.where(q_mat[state] == max_val)\n",
    "    \n",
    "    #get possible actions\n",
    "    possible_actions = adj_mat[state]\n",
    "    \n",
    "    #choose random value from the intersection of the two\n",
    "    return np.random.choice(np.intersect1d(possible_actions, max_indices))\n",
    "    \n",
    "\n",
    "    \n",
    "def normalize_matrix(matrix):\n",
    "    return matrix/float(np.max(matrix))\n",
    "\n",
    "def test_algo(start_room):\n",
    "    status = 1 #as long as room 5 is not found\n",
    "    max_steps = 20\n",
    "    step = 20\n",
    "    while status:\n",
    "        next_room = q_mat[start_room].argmax(axis=0)\n",
    "        print 'start_room:', start_room\n",
    "        print 'next_room:', next_room\n",
    "        start_room = next_room\n",
    "        print '\\n'\n",
    "        step += 1\n",
    "        if next_room == 5:\n",
    "            status = 0\n",
    "        if step == max_steps:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init q_matrix\n",
    "q_mat = np.zeros((6,6))\n",
    "\n",
    "#loop for learning the q_Table\n",
    "num_steps = 10\n",
    "epochs = 500\n",
    "max_steps = 60\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    status = 1 #while room 5 haven't been found\n",
    "\n",
    "    #get random room to start\n",
    "    curr_room = np.random.randint(0,6)\n",
    "    step = 0\n",
    "    #traverse until hit the goal\n",
    "    while status:\n",
    "        #if fall here, use the best route found so far\n",
    "        #to learn the q_table\n",
    "        if np.random.random() < lr:\n",
    "            action = find_best_action(curr_room)\n",
    "            update_q(curr_room, action)\n",
    "        else: #get a new room\n",
    "            possible_actions = adj_mat[curr_room]\n",
    "            action = np.random.choice(possible_actions)\n",
    "            update_q(curr_room, action)\n",
    "        curr_room = action            \n",
    "        step += 0\n",
    "        \n",
    "        #if current room is 5, found goal\n",
    "        if curr_room == 5:\n",
    "            status = 0\n",
    "        if step == max_steps:\n",
    "            print 'max steps reached'\n",
    "            break\n",
    "q_mat = normalize_matrix(q_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.8       ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.63990302,  0.        ,\n",
       "         1.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.64      ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.79996677,  0.512     ,  0.        ,  0.8       ,\n",
       "         0.        ],\n",
       "       [ 0.63995495,  0.        ,  0.        ,  0.64      ,  0.        ,\n",
       "         1.        ],\n",
       "       [ 0.        ,  0.79961889,  0.        ,  0.        ,  0.79998523,\n",
       "         0.99998154]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_room: 3\n",
      "next_room: 4\n",
      "\n",
      "\n",
      "start_room: 4\n",
      "next_room: 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_algo(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network implementation of Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_action(state, action):\n",
    "    cur_room = np.where(state == 1)[0][0]\n",
    "    if action not in adj_mat[cur_room]:\n",
    "        #impossible action. \n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def get_reward(state):\n",
    "    cur_room = np.where(state==1)[0][0]\n",
    "    if cur_room == 5:\n",
    "        return 10\n",
    "    else:\n",
    "        return -1\n",
    "        \n",
    "def init_state(seed=None):\n",
    "    state = np.zeros(6)\n",
    "    if seed is None:\n",
    "        rand_room = np.random.randint(0,6)\n",
    "    else:\n",
    "        rand_room = seed\n",
    "    state[rand_room] = 1\n",
    "    return state\n",
    "\n",
    "def test_network(model, debug=False):\n",
    "    max_steps = 15\n",
    "    for i in range(0, 6):\n",
    "        state = init_state(i)\n",
    "        print 'starting room:', i\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            q_vals = model.predict(state.reshape(1,6))\n",
    "            action = np.argmax(q_vals)\n",
    "            print 'start_room:', np.argmax(state)\n",
    "            print 'action:', action\n",
    "\n",
    "            if step > max_steps:\n",
    "                print 'max steps reached\\n'\n",
    "                break\n",
    "            state = init_state(action)\n",
    "            \n",
    "            reward = get_reward(state)\n",
    "            \n",
    "            if reward != -1:\n",
    "                print 'done\\n'\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#1 hot encoding of rooms\n",
    "model.add(Dense(128, input_shape=(6,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#output layer. 6 nodes for the 6 rooms\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "rms = RMSprop()\n",
    "model.compile(loss='mse', optimizer='sgd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,670\n",
      "Trainable params: 1,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "#variables\n",
    "epochs = 1000\n",
    "max_steps = 20#15\n",
    "epsilon = 0.8\n",
    "gamma = 0.7\n",
    "\n",
    "verbose=0\n",
    "debug = False\n",
    "if debug:\n",
    "    epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #set up room vector encoding\n",
    "    state = init_state()\n",
    "    step = 0\n",
    "    if debug:\n",
    "        print '\\n'\n",
    "        print 'init:', np.argmax(state)\n",
    "    while True:\n",
    "        if debug:\n",
    "            print 'current state:', state\n",
    "        \n",
    "        step += 1\n",
    "        #get the next action\n",
    "        qval = model.predict(state.reshape(1,6))\n",
    "        if debug:\n",
    "            print 'initial qval:', qval\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.argmax(qval)\n",
    "        \n",
    "        else:\n",
    "            action = np.random.randint(0,6)\n",
    "        \n",
    "        if debug:\n",
    "            print 'action:', action\n",
    "        #check if valid action\n",
    "        if not is_action(state, action):\n",
    "            if debug:\n",
    "                print 'bad target'\n",
    "            output = np.zeros((1,6)) #adjust shape for network input\n",
    "            output = qval\n",
    "            output[0][action] = -5 #this action shouldn't happen\n",
    "            \n",
    "            #fit the model to learn this choice\n",
    "            model.fit(state.reshape(1,6), output,\n",
    "                     epochs=1, verbose=0)\n",
    "        \n",
    "        else: #good next action\n",
    "            if debug:\n",
    "                print 'good target'\n",
    "            #create new state to calculate Q(s', a)\n",
    "            new_state = np.zeros(6)\n",
    "            new_state[action] = 1\n",
    "            if debug:\n",
    "                print 'new state:', new_state\n",
    "            #get reward\n",
    "            reward = get_reward(new_state)\n",
    "            \n",
    "            #get maxQ of next state\n",
    "            new_q = model.predict(new_state.reshape(1,6))\n",
    "            max_q = np.max(new_q)\n",
    "            \n",
    "            #create target variable\n",
    "            output = np.zeros((1,6))\n",
    "            output = qval\n",
    "            if reward <= 0: #non-terminal state\n",
    "                update = reward + (gamma * max_q)\n",
    "            else:\n",
    "                update = reward\n",
    "            output[0][action] = update\n",
    "            \n",
    "            #fit the desired output to the model\n",
    "            model.fit(new_state.reshape(1,6), output, \n",
    "                     epochs=1, verbose=verbose)\n",
    "            state = new_state\n",
    "            \n",
    "            if reward > 0: #found terminal room\n",
    "                break\n",
    "        if debug:\n",
    "            print 'new target:', output\n",
    "            new_q = model.predict(state.reshape(1,6))\n",
    "            print 'new q_val:', new_q\n",
    "            print '-----------'\n",
    "        #clear_output(wait=True)\n",
    "        #check if terminal room reached, or max_steps reached\n",
    "        if step > max_steps:\n",
    "            if debug:\n",
    "                print 'max steps reached'\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11752021,  0.16640402,  0.17932314,  0.17418927,  0.1814322 ,\n",
       "         0.18113114]], dtype=float32)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array([1,0,0,0,0,0])\n",
    "model.predict(state.reshape(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting room: 0\n",
      "start_room: 0\n",
      "action: 4\n",
      "start_room: 4\n",
      "action: 5\n",
      "done\n",
      "\n",
      "starting room: 1\n",
      "start_room: 1\n",
      "action: 5\n",
      "done\n",
      "\n",
      "starting room: 2\n",
      "start_room: 2\n",
      "action: 3\n",
      "start_room: 3\n",
      "action: 4\n",
      "start_room: 4\n",
      "action: 5\n",
      "done\n",
      "\n",
      "starting room: 3\n",
      "start_room: 3\n",
      "action: 4\n",
      "start_room: 4\n",
      "action: 5\n",
      "done\n",
      "\n",
      "starting room: 4\n",
      "start_room: 4\n",
      "action: 5\n",
      "done\n",
      "\n",
      "starting room: 5\n",
      "start_room: 5\n",
      "action: 5\n",
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_network(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
