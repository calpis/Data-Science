{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import functions\n",
    "reload(functions)\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "#viz\n",
    "from datetime import date\n",
    "from random import randint\n",
    "import math\n",
    "\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "from bokeh.io import output_file, show\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from dateutil import parser\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import arma_order_select_ic\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#from bokeh.io import vform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##variables\n",
    "look_back = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = '/Users/momori/Documents/projects/log_monitoring/muleprep_app_ERROR.csv'\n",
    "log_file1 = '/Users/momori/Documents/projects/log_monitoring/muleprod_app_aug_7.csv'\n",
    "log_file2 = '/Users/momori/Documents/projects/log_monitoring/muleprod_app_aug_1.csv'\n",
    "log_file3 = '/Users/momori/Documents/projects/log_monitoring/muleprep_app_ERROR.csv'\n",
    "log_file4 = '/Users/momori/Documents/projects/log_monitoring/muleprep_app_ERROR.csv'\n",
    "\n",
    "df_test = pd.read_csv(log_file)\n",
    "df_train1 = pd.read_csv(log_file)#1)\n",
    "df_train2 = pd.read_csv(log_file)#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = [df_train1, df_train2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##km: clustering model\n",
    "##vecetorizer: document vectorizer\n",
    "##document: document to cluster\n",
    "def predict_document_cluster(km, vectorizer, document):\n",
    "    doc = vectorizer.transform([document])\n",
    "    return km.predict(doc)\n",
    "\n",
    "def format_df(df):\n",
    "    r_df = df.dropna(axis=1)\n",
    "    r_df.loc[:,'_time'] = r_df['_time'].apply(lambda x: format_time(x))\n",
    "    return r_df\n",
    "\n",
    "def format_time(string):\n",
    "    ##may need smarter logic. seems slow\n",
    "    return parser.parse(string)\n",
    "\n",
    "##step_time in minutes\n",
    "##\n",
    "## returns: dict. key is timestamp, value is dataframe\n",
    "def split_by_time(df, step_time=30):\n",
    "    r_dict = {}\n",
    "    t=str(step_time)+'Min'\n",
    "    \n",
    "    #set _time to index\n",
    "    tmp = df.set_index(1)\n",
    "    tmp = tmp.groupby(pd.TimeGrouper(freq=t))\n",
    "    for key, item in tmp:\n",
    "        try:\n",
    "            r_dict[str(key)] = tmp.get_group(key)\n",
    "        except:\n",
    "            continue\n",
    "    return r_dict\n",
    "        \n",
    "def show_top_words(km, vect, show=False, save=True, out_filename='output'):\n",
    "    dcluster_words = {}\n",
    "    if show:\n",
    "        print(\"Top terms per cluster:\")\n",
    "\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vect.get_feature_names()\n",
    "    for i in range(k):\n",
    "        if show:\n",
    "            print(\"Cluster %d:\" % i)\n",
    "            for ind in order_centroids[i, :10]:\n",
    "                print(' %s' % terms[ind])\n",
    "        if save:\n",
    "            words = []\n",
    "            for ind in order_centroids[i, :10]:\n",
    "                words.append(terms[ind])\n",
    "            dcluster_words[i] = words\n",
    "    if save:\n",
    "        np.save(out_filename, dcluster_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb               module_test.ipynb\r\n",
      "Untitled1.ipynb              new_df_formatted\r\n",
      "bollinger.html               new_logs\r\n",
      "data_table.html              output.npy\r\n",
      "functions.ipynb              splunk log analyzer v2.ipynb\r\n",
      "line_bar.html                splunk log analyzer.ipynb\r\n",
      "log_analysis_tutorial.ipynb  splunk_log_analyzer_v3.ipynb\r\n",
      "log_cleaning.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "#show_top_words(km, vect, save=True)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [u'320bbec0', u'55', u'b667', u'42ff', u'd7c5e8e7d50a', u'f96e', u'29', u'6335', u'account', u'enterpriseaccount']\n",
      "1 [u'order', u'avs', u'auth', u'payment', u'service', u'errors', u'code', u'failed', u'type', u'invalid']\n",
      "2 [u'entitlement', u'entitlementofferingrestransformer', u'entitledoffering', u'transformer', u'5266', u'received', u'46', u'connectorwithoutmulesession', u'45', u'02']\n",
      "3 [u'provided', u'customeraccountnumber', u'invalid', u'moreinfo', u'errors', u'code', u'type', u'202106', u'ent', u'entitledofferings']\n",
      "4 [u'java', u'org', u'mule', u'extendedschemavalidationfilter', u'runtimeexception', u'wrapper', u'222', u'lang', u'exceptions', u'schemavalidation']\n",
      "5 [u'00', u'currency', u'usd', u'ns1', u'globalsalestax', u'productclass', u'totalcredit', u'salestaxheader', u'documentid', u'taxtype']\n",
      "6 [u'licensenumber', u'missing', u'parameter', u'required', u'errors', u'202103', u'code', u'type', u'ent', u'entitledofferings']\n",
      "7 [u'multiple', u'schemavalidationfilter', u'rejected', u'validate', u'apparently', u'globalsalestax', u'schemavalidation', u'extendedschemavalidationfilter', u'2823', u'failed']\n",
      "8 [u'6dc10132', u'78dc', u'50ad', u'4448', u'37', u'350253329de3', u'6335', u'account', u'27', u'enterpriseaccount']\n",
      "9 [u'net', u'java', u'org', u'true', u'80', u'factory', u'endpoint', u'session', u'transport', u'false']\n",
      "10 [u'payment', u'information', u'auth', u'service', u'order', u'invalid', u'failed', u'errors', u'code', u'type']\n",
      "11 [u'rps', u'restrictions', u'ebcs', u'httpnosessionconnector', u'4628', u'09', u'25', u'031', u'postalcode', u'country']\n",
      "12 [u'6323', u'304', u'31', u'billingprofiles', u'10', u'jettyconnector', u'02', u'util', u'transformer', u'service']\n",
      "13 [u'619', u'35', u'5296', u'40', u'processor', u'loggermessageprocessor', u'service', u'org', u'api', u'connectorwithoutmulesession']\n",
      "14 [u'entitlement', u'entitlementofferingrestransformer', u'entitledoffering', u'transformer', u'received', u'connectorwithoutmulesession', u'cas', u'45', u'02', u'5309']\n",
      "15 [u'code', u'order', u'errors', u'type', u'jettyconnector', u'invalid', u'49', u'server', u'standalone', u'yes']\n",
      "16 [u'b8f0da82', u'6568', u'371b', u'695c', u'9bd2', u'f8f30e8ad4bd', u'account', u'29', u'27', u'enterpriseaccount']\n",
      "17 [u'failure', u'mdm', u'mule', u'processor', u'loggermessageprocessor', u'enterpriseaccount', u'received', u'api', u'org', u'53']\n",
      "18 [u'rejected', u'multiple', u'15', u'schemavalidationfilter', u'validate', u'apparently', u'36', u'globalsalestax', u'schemavalidation', u'extendedschemavalidationfilter']\n",
      "19 [u'1b6f2c45d63d', u'870a', u'6355', u'fdc2', u'a5f1', u'51', u'3b01a6a7', u'34', u'account', u'enterpriseaccount']\n"
     ]
    }
   ],
   "source": [
    "d = np.load('output.npy').item()\n",
    "for k,v1 in d.iteritems():\n",
    "    print k,v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_train1 = df_train1['_raw'].tolist()\n",
    "logs_train2 = df_train2['_raw'].tolist()\n",
    "logs_test = df_test['_raw'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Fit a vectorizer\n",
    "vect = TfidfVectorizer(max_df = 0.5, min_df = 2, stop_words='english')\n",
    "\n",
    "#v is base vectorizer, can be used to transform other logs\n",
    "v = vect.fit(logs_train1)\n",
    "\n",
    "#x is the tfidf sparse matrix created for this set of logs\n",
    "x1 = v.transform(logs_train1)\n",
    "x2 = v.transform(logs_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create kmeans model and fit\n",
    "k=20\n",
    "km = MiniBatchKMeans(n_clusters=k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000)\n",
    "km.fit(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Let's use cluster 6 as it shows '500' as one of the top words\n",
    "show_top_words(km, vect, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in range(20):\n",
    "#    print i, len(df_predict1[df_predict1[2]==i]), len(df_predict2[df_predict2[2]==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get predictions for a subset of the logs and create df with _raw, _time, and label\n",
    "formatted_df = format_df(df_train1)\n",
    "\n",
    "logs = formatted_df['_raw'].tolist()\n",
    "times = formatted_df['_time'].tolist()\n",
    "test_x1 = v.transform(logs)\n",
    "\n",
    "labels = km.predict(test_x1)\n",
    "lst = zip(logs, times, labels.tolist())\n",
    "df_predict1 = pd.DataFrame(lst)\n",
    "df_predict1.head()\n",
    "\n",
    "##do same for second train set\n",
    "formatted_df = format_df(df_train2)\n",
    "\n",
    "logs = formatted_df['_raw'].tolist()\n",
    "times = formatted_df['_time'].tolist()\n",
    "test_x2 = v.transform(logs)\n",
    "\n",
    "labels = km.predict(test_x2)\n",
    "lst = zip(logs, times, labels.tolist())\n",
    "df_predict2 = pd.DataFrame(lst)\n",
    "df_predict2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##for each cluster number, create a dt\n",
    "df_clusters1, df_clusters2 = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_num = 8\n",
    "f_df1 = df_predict1[df_predict1[2]==cluster_num]\n",
    "f_df2 = df_predict2[df_predict2[2]==cluster_num]\n",
    "#f_df1 = df_predict1\n",
    "#f_df2 = df_predict2\n",
    "f_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_logs = f_df1[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Create dictionary of dataframes, split by time\n",
    "d1 = OrderedDict(sorted(split_by_time(f_df1).items()))\n",
    "d2 = OrderedDict(sorted(split_by_time(f_df2).items()))\n",
    "#d1 = OrderedDict(sorted(split_by_time(df_predict1).items()))\n",
    "#d2 = OrderedDict(sorted(split_by_time(df_predict2).items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create counts for training\n",
    "counts1 = []\n",
    "counts2 = []\n",
    "for k, val in d1.iteritems():\n",
    "    counts1.append(val.shape[0])\n",
    "for k, val in d2.iteritems():\n",
    "    counts2.append(val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(counts1), len(counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts1[:10], counts2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = vect.fit(logs_train1)\n",
    "v.transform(logs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Let's fit the 'future' data and pull out cluster_num\n",
    "formatted_df = format_df(df_test)\n",
    "logs_test = formatted_df['_raw'].tolist()\n",
    "times = formatted_df['_time'].tolist()\n",
    "test_x1 = v.transform(logs_test)\n",
    "\n",
    "labels = km.predict(test_x1)\n",
    "lst = zip(logs, times, labels.tolist())\n",
    "test_data = pd.DataFrame(lst)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data[test_data[2]==cluster_num]\n",
    "\n",
    "#test_data = test_data\n",
    "test_d = OrderedDict(sorted(split_by_time(test_data).items()))\n",
    "test_counts = []\n",
    "for k, val in test_d.iteritems():\n",
    "    test_counts.append(val.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#zip the data\n",
    "counts1 = [float(i) for i in counts1]\n",
    "counts2 = [float(i) for i in counts2]\n",
    "\n",
    "train = zip(counts1, counts2)\n",
    "train = [i for i in train if math.fabs(i[1]-i[0] < 100)]\n",
    "print len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##make predictions by fitting ARIMA model into the lists\n",
    "prediction_upperbound = []\n",
    "prediction_lowerbound = []\n",
    "prediction_value = []\n",
    "\n",
    "for t in train:\n",
    "    rgr = ARIMA(t, order=(0,0,0))\n",
    "    fit = rgr.fit(disp=0)\n",
    "    forecast =  fit.forecast()\n",
    "    prediction_upperbound.append(forecast[2][0][1])\n",
    "    prediction_lowerbound.append(forecast[2][0][0])\n",
    "    prediction_value.append(forecast[0])\n",
    "\n",
    "#Turn into numpy array for computation\n",
    "prediction_upperbound = np.asarray(prediction_upperbound)\n",
    "prediction_lowerbound = np.asarray(prediction_lowerbound)\n",
    "prediction_value = np.asarray(prediction_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "\n",
    "# Define Bollinger Bands.\n",
    "upperband = prediction_upperbound\n",
    "lowerband = prediction_lowerbound\n",
    "x_data = np.arange(1, 101)\n",
    "\n",
    "length = min(len(prediction_upperbound), len(prediction_lowerbound))\n",
    "x_data = np.arange(1, length)\n",
    "# Bollinger shading glyph:\n",
    "band_x = np.append(x_data, x_data[::-1])\n",
    "band_y = np.append(lowerband, upperband[::-1])\n",
    "\n",
    "output_file('bollinger.html', title='Bollinger bands (file)')\n",
    "\n",
    "\n",
    "p = figure(x_axis_type='datetime', title=\"\")\n",
    "\n",
    "# add a line renderer\n",
    "p.line([i for i in range(1,length)], counts2, line_width=2)\n",
    "p.grid.grid_line_alpha = 0.4\n",
    "p.x_range.range_padding = 0\n",
    "p.plot_height = 600\n",
    "p.plot_width = 800\n",
    "\n",
    "p.patch(band_x, band_y, color='#7570B3', fill_alpha=0.2)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = '/Users/momori/PycharmProjects/AIMonitoring/data/qq.csv'\n",
    "dd = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##get predictions for a subset of the logs and create df with _raw, _time, and label\n",
    "test_df = format_df(df[0:10000])\n",
    "\n",
    "logs = test_df['_raw'].tolist()\n",
    "times = test_df['_time'].tolist()\n",
    "test_x = vect.transform(logs)\n",
    "\n",
    "labels = km.predict(test_x)\n",
    "lst = zip(logs,times,labels.tolist())\n",
    "df_predict = pd.DataFrame(lst)\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Create dictionary of dataframes, split by time\n",
    "d = split_by_time(df_predict)\n",
    "for k,v in d.iteritems():\n",
    "    print k, ':::', type(v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d['2017-08-19 13:00:00-07:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Cluster the interval logs by using km.  \n",
    "#Then get the count for predicting outcomes.\n",
    "\n",
    "#test out on d\n",
    "counts = []\n",
    "for k, v in d.iteritems():\n",
    "    print k\n",
    "    counts.append(v.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##train gradient boosted trees for regression\n",
    "params = {'n_estimators': 10, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use an ARIMA Model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "floats = [float(i) for i in counts]\n",
    "floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgr = ARIMA(floats[:len(floats)-1], order=(0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = rgr.fit(disp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit.forecast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "\n",
    "# Define Bollinger Bands.\n",
    "upperband = np.random.random_integers(100, 150, size=100)\n",
    "lowerband = upperband - 100\n",
    "x_data = np.arange(1, 101)\n",
    "\n",
    "# Bollinger shading glyph:\n",
    "band_x = np.append(x_data, x_data[::-1])\n",
    "band_y = np.append(lowerband, upperband[::-1])\n",
    "\n",
    "output_file('bollinger.html', title='Bollinger bands (file)')\n",
    "\n",
    "\n",
    "p = figure(x_axis_type='datetime', title=\"Bollinger Bands\")\n",
    "\n",
    "# add a line renderer\n",
    "p.line([0, 20, 30, 40, 50], [80, 130, 135, 90, 90], line_width=4)\n",
    "p.grid.grid_line_alpha = 0.4\n",
    "p.x_range.range_padding = 0\n",
    "p.plot_height = 600\n",
    "p.plot_width = 800\n",
    "\n",
    "p.patch(band_x, band_y, color='#7570B3', fill_alpha=0.2)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from bokeh.plotting import figure, output_file, show\n",
    "# from bokeh.models.ranges import Range1d\n",
    "# import numpy\n",
    "\n",
    "\n",
    "# output_file(\"line_bar.html\")\n",
    "\n",
    "# p = figure(plot_width=400, plot_height=400)\n",
    "\n",
    "# # add a line renderer\n",
    "# p.line([1, 2, 3, 4, 5], [6, 7, 6, 4, 5], line_width=2)\n",
    "\n",
    "# # setting bar values\n",
    "# h = numpy.array([2, 8, 5, 10, 7])\n",
    "\n",
    "# # Correcting the bottom position of the bars to be on the 0 line.\n",
    "# adj_h = h/2\n",
    "\n",
    "# # add bar renderer\n",
    "# p.rect(x=[1, 2, 3, 4, 5], y=adj_h, width=0.4, height=h, color=\"#CAB2D6\")\n",
    "\n",
    "# # Setting the y  axis range   \n",
    "# p.y_range = Range1d(0, 12)\n",
    "\n",
    "# #p.title = \"Line and Bar\"\n",
    "\n",
    "# show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from bokeh.plotting import figure, output_file, show\n",
    "# from bokeh.models.ranges import Range1d\n",
    "# import numpy\n",
    "\n",
    "\n",
    "# output_file(\"line_bar.html\")\n",
    "# p = figure(plot_width=400, plot_height=400)\n",
    "\n",
    "# p.line([1, 2, 3, 4, 5], [6, 7, 6, 4, 5], line_width=2)\n",
    "\n",
    "\n",
    "# show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
