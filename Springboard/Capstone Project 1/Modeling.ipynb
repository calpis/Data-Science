{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Keras on Amazon's fine food review\n",
    "\n",
    "This notebook will create a convolutional neural network (CNN) off of the fine food review to suggest if the review is a positive (4 or 5 star rating) or a non-positive (1, 2, or 3 star rating).  What we aim here is to use the word2vec embedding implementation in Keras to convert the text into word vectors, and train a CNN with it which will predict, given a review text whethe or not the rating is positive or non-positive.\n",
    "\n",
    "The steps taken during this notebook are the follows:<br>\n",
    "- Create a column to indicate if the score was positive or negative, this will be called positive_review\n",
    "- The text will be turned into vectors using keras implementation of word2vec\n",
    "- distribution of the vectors will be analyzed to check for outliers and discard them as necessary.\n",
    "- short vectors will be 0-padded to match in length to the longer vectors.\n",
    "- CNNs will be trained using different parameters and performance compared amongst the other CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data importing/wrangling\n",
    "import pandas as pd\n",
    "\n",
    "#text processing \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#CNN modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, Conv1D, MaxPooling1D, MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "#data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "input_location = '/Users/momori/Downloads/amazon-fine-food-reviews/reviews_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(input_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'Id', u'ProductId', u'UserId', u'ProfileName',\n",
       "       u'HelpfulnessNumerator', u'HelpfulnessDenominator', u'Score', u'Time',\n",
       "       u'Summary', u'Text', u'HelpfulnessRatio', u'avg_score',\n",
       "       u'normalized_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column 'positive_review' which indicates if the review was positive (4,5 stars) or non-positive (1,2,3 stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add a positive/non-positive rating oolumn\n",
    "data['positive_review'] = data['Score'] > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create documents and labels to train the model later\n",
    "docs = data['Text']\n",
    "labels = data['positive_review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embedding\n",
    "\n",
    "Here we will use the word2vec embedding algorithm implementation by keras.  Since neural networks can only take in numerical values and vectors to train from, the texts will need to be encoded into numerical values.  After each review is encoded, the lengths of the review will be analyzed because the shorter review vedctors needs to be 0-padded to match the longest review vector.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 17, 125, 319, 7, 1, 4940, 523, 103, 52, 204, 3, 17, 117, 28, 41, 5, 30, 7, 29, 183, 1, 38, 629, 48, 26, 4, 2636, 58, 4, 1183, 448, 3, 6, 619, 99, 13, 5266, 8, 1777, 3, 94, 8695, 9, 38, 99, 58, 140]\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec embedding\n",
    "\n",
    "#use the tokenizer API provided by Keras to turn documents into sequences\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "#encode the documents into integers\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a list of lengths of each of the encoded_docs to find out the longest length, \n",
    "#and see if any 0 padding is required\n",
    "doc_lengths = [len(doc) for doc in encoded_docs]\n",
    "max_doc_length = max(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  percentile of document length: 57.0\n",
      "60  percentile of document length: 70.0\n",
      "70  percentile of document length: 88.0\n",
      "80  percentile of document length: 114.0\n",
      "90  percentile of document length: 164.0\n",
      "99  percentile of document length: 398.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAAFJCAYAAAAWph3tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105943e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's put the lengths into a boxplot to see the distributions of the lengths\n",
    "np_array = np.array(doc_lengths)\n",
    "\n",
    "for i in [50,60,70,80,90,99]:\n",
    "    print i, \" percentile of document length:\", np.percentile(np_array,i)\n",
    "    \n",
    "ax = sns.boxplot(x=doc_lengths, showfliers=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see from the above plot that the third quantile of document lengths is around 200 words or so.  We can test using this as the max document length and ignoring the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFJCAYAAACyzKU+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHkBJREFUeJzt3X9M3dXh//HX/QHlXizRbJHrRVjroPyouUD4pMHF9Y+6\nsbHP7B+4dnczadowNh3OGqvTqIzS2kgTY5ZW5lqWleo0yxZWZ/JJN3ajxqWLiwRGkYCIUOaKhHZd\ni3rv7eX2vr9/+O2dtbT8KHA5b5+Pv+Twfst59dx7X5w394fDsixLAADAGM5UTwAAAMwN5Q0AgGEo\nbwAADEN5AwBgGMobAADDUN4AABiG8gYAwDDuVE9gLsbGxuZ8jt/vn9d5y5Wd8tgpi2SvPHbKItkr\nj52ySPbKsxhZ/H7/tOPsvAEAMAzlDQCAYShvAAAMQ3kDAGAYyhsAAMNQ3gAAGIbyBgDAMJQ3AACG\nobwBADAM5Q0AgGEobwAADEN5AwBgGKM+mOTz5kLdxsvG3p/Fea7WVxZ+MgCAZYOdNwAAhqG8AQAw\nDOUNAIBhKG8AAAxDeQMAYBjKGwAAw1DeAAAYhvIGAMAwlDcAAIahvAEAMAzlDQCAYShvAAAMM6sP\nJjly5Ig6OzsVj8f1jW98QyUlJWppaZHD4VBubq5qa2vldDoVCoUUCoXkcrlUU1OjiooKxWIx7du3\nT5OTk/J4PKqvr1dWVpYGBwfV1tYml8ulQCCgTZs2LXZWAABsYcadd19fn9555x3t3r1bTU1NOn36\ntA4fPqxgMKhdu3bJsix1dnbq7NmzOnr0qHbv3q3HH39cL730kqamptTR0aG8vDzt2rVL69evV3t7\nuySptbVV999/v3bt2qWhoSGNjIwselgAAOxgxvLu6elRXl6enn76ae3du1cVFRUaHh5WSUmJJKm8\nvFzHjx/X0NCQCgsLlZaWJq/XK5/Pp9HRUQ0MDKisrCx5bG9vr8LhsOLxuHw+nxwOh0pLS9Xb27u4\nSQEAsIkZL5tPTk7q9OnTevTRRzUxMaG9e/fKsiw5HA5JksfjUTgcVjgcltfrTZ53cTwSiSTHMzIy\nkmMejyd5bEZGhiYmJmacrN/vn3PAazkv1Wbz2d3TMSmvSXOdDTvlsVMWyV557JRFsleepcoyY3mv\nXLlSOTk5crvd8vv9Sk9P17///e/k9yORiDIzM+X1ehWNRi8b93g8yfFoNJoci0QiyWOj0eglxX8l\nY2NjcwonffIPOZ/zTGZKXrutjZ3y2CmLZK88dsoi2SvPYmS50i8DM142Lyoq0j/+8Q9ZlqUzZ84o\nGo3q1ltvVV9fnySpu7tbxcXFys/PV39/v2KxmMLhsE6ePKnc3FwVFhaqq6sreWxRUZG8Xq/cbrfG\nx8dlWZZ6enpUXFy8gHEBALCvGXfeFRUV6u/v12OPPaZEIqHa2lrdeOONOnDggOLxuHJyclRZWSmn\n06nq6mo1NjYqkUgoGAwqPT1dVVVVamlpUUNDg9xut7Zv3y5Jqqur0/79+5VIJBQIBFRQULDoYQEA\nsAOHZVlWqicxW5+3y+YX6jbO6zxX6ysLPJPFYfLaTMdOeeyURbJXHjtlkeyVZ1ldNgcAAMsL5Q0A\ngGEobwAADEN5AwBgGMobAADDUN4AABiG8gYAwDCUNwAAhqG8AQAwDOUNAIBhKG8AAAxDeQMAYBjK\nGwAAw1DeAAAYhvIGAMAwlDcAAIahvAEAMAzlDQCAYShvAAAMQ3kDAGAYyhsAAMNQ3gAAGIbyBgDA\nMJQ3AACGobwBADAM5Q0AgGEobwAADEN5AwBgGMobAADDUN4AABiG8gYAwDCUNwAAhqG8AQAwDOUN\nAIBhKG8AAAxDeQMAYBjKGwAAw7hnc9Ajjzwij8cjSbrxxhtVU1OjlpYWORwO5ebmqra2Vk6nU6FQ\nSKFQSC6XSzU1NaqoqFAsFtO+ffs0OTkpj8ej+vp6ZWVlaXBwUG1tbXK5XAoEAtq0adOiBgUAwC5m\nLO9YLCbLsrRz587k2N69exUMBrV27VodPHhQnZ2dWrNmjY4eParm5mZNTU2poaFBgUBAHR0dysvL\n0+bNm3Xs2DG1t7dr27Ztam1t1Y4dO5Sdna3m5maNjIxo9erVi5kVAABbmLG8R0dHdf78eT355JO6\ncOGCvve972l4eFglJSWSpPLycvX09MjpdKqwsFBpaWlKS0uTz+fT6OioBgYGtHHjxuSx7e3tCofD\nisfj8vl8kqTS0lL19vZS3gAAzMKM5b1ixQrdeeeduuOOO/TBBx/oqaeekiQ5HA5JksfjUTgcVjgc\nltfrTZ53cTwSiSTHMzIykmMXL8NfHJ+YmJhxsn6/f27prvG8VHt/nueZlNekuc6GnfLYKYtkrzx2\nyiLZK89SZZmxvG+66Sb5fD45HA75/X5dd911Gh4eTn4/EokoMzNTXq9X0Wj0snGPx5Mcj0ajybFI\nJJI8NhqNXlL8VzI2NjancNIn/5DzOc9kpuS129rYKY+dskj2ymOnLJK98ixGliv9MjDjs81fe+01\nPf/885KkM2fOKBKJqLS0VH19fZKk7u5uFRcXKz8/X/39/YrFYgqHwzp58qRyc3NVWFiorq6u5LFF\nRUXyer1yu90aHx+XZVnq6elRcXHxQmUFAMDWZtx5b9iwQS0tLWpoaJDD4dC9996rlStX6sCBA4rH\n48rJyVFlZaWcTqeqq6vV2NioRCKhYDCo9PR0VVVVJc93u93avn27JKmurk779+9XIpFQIBBQQUHB\noocFAMAOHJZlWamexGx93i6bX6jbOK/zXK2vLPBMFofJazMdO+WxUxbJXnnslEWyV55lddkcAAAs\nL5Q3AACGmdU7rMH+7H6JHgDshJ03AACGYedtQ/PdRQMAzMDOGwAAw1DeAAAYhvIGAMAwlDcAAIah\nvAEAMAzlDQCAYShvAAAMQ3kDAGAYyhsAAMNQ3gAAGIbyBgDAMJQ3AACGobwBADAM5Q0AgGEobwAA\nDEN5AwBgGMobAADDUN4AABiG8gYAwDCUNwAAhqG8AQAwDOUNAIBhKG8AAAxDeQMAYBjKGwAAw1De\nAAAYhvIGAMAwlDcAAIahvAEAMAzlDQCAYShvAAAMQ3kDAGAY92wOOnfunB599FE98cQTcrlcamlp\nkcPhUG5urmpra+V0OhUKhRQKheRyuVRTU6OKigrFYjHt27dPk5OT8ng8qq+vV1ZWlgYHB9XW1iaX\ny6VAIKBNmzYtdk4AAGxjxp13PB7XwYMHlZ6eLkk6fPiwgsGgdu3aJcuy1NnZqbNnz+ro0aPavXu3\nHn/8cb300kuamppSR0eH8vLytGvXLq1fv17t7e2SpNbWVt1///3atWuXhoaGNDIysrgpAQCwkRnL\n+4UXXtDXv/513XDDDZKk4eFhlZSUSJLKy8t1/PhxDQ0NqbCwUGlpafJ6vfL5fBodHdXAwIDKysqS\nx/b29iocDisej8vn88nhcKi0tFS9vb2LGBEAAHu56mXz119/XVlZWSorK9PLL7+cHHc4HJIkj8ej\ncDiscDgsr9eb/P7F8UgkkhzPyMhIjnk8nuSxGRkZmpiYmNVk/X7/7JMtwHmp9n6qJzAL1/pva+ra\nXImd8tgpi2SvPHbKItkrz1JluWp5v/baa5Kk3t5enThxQs8++6zOnTuX/H4kElFmZqa8Xq+i0ehl\n4x6PJzkejUaTY5FIJHlsNBq9pPivZmxsbPbJ/j+/3z+v8zA71/Jva7e1sVMeO2WR7JXHTlkke+VZ\njCxX+mXgqpfNm5qa1NTUpJ07d2rVqlW67777VFZWpr6+PklSd3e3iouLlZ+fr/7+fsViMYXDYZ08\neVK5ubkqLCxUV1dX8tiioiJ5vV653W6Nj4/Lsiz19PSouLh4QcMCAGBns3q2+adt2bJFBw4cUDwe\nV05OjiorK+V0OlVdXa3GxkYlEgkFg0Glp6erqqpKLS0tamhokNvt1vbt2yVJdXV12r9/vxKJhAKB\ngAoKChY8GAAAduWwLMtK9SRm6/N22fxC3cZUT2FGrtZX5n2uyWszHTvlsVMWyV557JRFsleeZXPZ\nHAAALD+UNwAAhqG8AQAwDOUNAIBhKG8AAAwz55eKAdfq4rPo5/oOctfyzHYAsBN23gAAGIbyBgDA\nMJQ3AACGobwBADAM5Q0AgGF4tjmuiQnvvw4AdkN5LxFKDgCwULhsDgCAYShvAAAMQ3kDAGAYyhsA\nAMNQ3gAAGIbyBgDAMJQ3AACGobwBADAM5Q0AgGEobwAADEN5AwBgGMobAADDUN4AABiG8gYAwDB8\nJCiMMZ+PVXW1vrIIMwGA1GLnDQCAYShvAAAMQ3kDAGAYyhsAAMNQ3gAAGIbyBgDAMJQ3AACGobwB\nADDMjG/Skkgk9Mtf/lIffPCBJKmurk7p6elqaWmRw+FQbm6uamtr5XQ6FQqFFAqF5HK5VFNTo4qK\nCsViMe3bt0+Tk5PyeDyqr69XVlaWBgcH1dbWJpfLpUAgoE2bNi16WAAA7GDG8u7s7JQk7d69W319\nffrtb38ry7IUDAa1du1aHTx4UJ2dnVqzZo2OHj2q5uZmTU1NqaGhQYFAQB0dHcrLy9PmzZt17Ngx\ntbe3a9u2bWptbdWOHTuUnZ2t5uZmjYyMaPXq1YseGAAA08142XzdunX60Y9+JEk6deqUvF6vhoeH\nVVJSIkkqLy/X8ePHNTQ0pMLCQqWlpcnr9crn82l0dFQDAwMqKytLHtvb26twOKx4PC6fzyeHw6HS\n0lL19vYuYkwAAOxjVu9t7nK59Oyzz+qtt97Sgw8+qN7eXjkcDkmSx+NROBxWOByW1+tNnnNxPBKJ\nJMczMjKSYx6PJ3lsRkaGJiYmZpyH3++fU7hrPW8hvZ/qCXxOLfXaL4fb2kKxUxbJXnnslEWyV56l\nyjLrDya57777dPbsWT322GOKxWLJ8UgkoszMTHm9XkWj0cvGPR5PcjwajSbHIpFI8thoNHpJ8V/J\n2NjYbKeb5Pf753Ue7GEp195OtzU7ZZHslcdOWSR75VmMLFf6ZWDGy+ZvvPGGjhw5IklKT0+Xw+HQ\nLbfcor6+PklSd3e3iouLlZ+fr/7+fsViMYXDYZ08eVK5ubkqLCxUV1dX8tiioiJ5vV653W6Nj4/L\nsiz19PSouLh4obICAGBrM+68161bp1/84hdqbGxUPB7X1q1blZOTowMHDigejysnJ0eVlZVyOp2q\nrq5WY2OjEomEgsGg0tPTVVVVpZaWFjU0NMjtdmv79u2SPnnW+v79+5VIJBQIBFRQULDoYQEAsAOH\nZVlWqicxWyZfNp/PZ1Hj2i3l53kvl9vaQrBTFsleeeyURbJXnmV12RwAACwvlDcAAIahvAEAMAzl\nDQCAYShvAAAMQ3kDAGAYyhsAAMNQ3gAAGIbyBgDAMJQ3AACGobwBADAM5Q0AgGEobwAADEN5AwBg\nGMobAADDUN4AABjGneoJAMvRhbqNcz/p/zoXfiIAMA123gAAGIadN2xtXjtoAFjm2HkDAGAYyhsA\nAMNQ3gAAGIbyBgDAMJQ3AACGobwBADAM5Q0AgGEobwAADEN5AwBgGMobAADDUN4AABiG8gYAwDCU\nNwAAhqG8AQAwDOUNAIBhKG8AAAxDeQMAYBjKGwAAw7iv9s14PK7nnntOp06d0tTUlO666y7dfPPN\namlpkcPhUG5urmpra+V0OhUKhRQKheRyuVRTU6OKigrFYjHt27dPk5OT8ng8qq+vV1ZWlgYHB9XW\n1iaXy6VAIKBNmzYtVV4AAIx31fL+61//qpUrV+onP/mJPvroIz388MNatWqVgsGg1q5dq4MHD6qz\ns1Nr1qzR0aNH1dzcrKmpKTU0NCgQCKijo0N5eXnavHmzjh07pvb2dm3btk2tra3asWOHsrOz1dzc\nrJGREa1evXqpMgMAYLSrlvdtt92myspKSZJlWXK5XBoeHlZJSYkkqby8XD09PXI6nSosLFRaWprS\n0tLk8/k0OjqqgYEBbdy4MXlse3u7wuGw4vG4fD6fJKm0tFS9vb2UNz63LtRtnPM5rtZXFmEmAExx\n1fLOyMiQJEUiET3zzDMKBoN64YUX5HA4JEkej0fhcFjhcFherzd53sXxSCSSHM/IyEiOeTyeS37G\nxMTErCbr9/vnlu4az1tI76d6AlgS87mtzee2sRS36eVwv1lIdspjpyySvfIsVZarlrcknT59Wk8/\n/bSqqqp0++236ze/+U3ye5FIRJmZmfJ6vYpGo5eNezye5Hg0Gk2ORSKR5LHRaPSS4r+asbGxWQe7\nyO/3z+s8YD6W6ra22D/HbvcbO+WxUxbJXnkWI8uVfhm46rPNz549qz179ujuu+/Whg0bJEmrVq1S\nX1+fJKm7u1vFxcXKz89Xf3+/YrGYwuGwTp48qdzcXBUWFqqrqyt5bFFRkbxer9xut8bHx2VZlnp6\nelRcXLyQWQEAsLWr7ryPHDmijz76SO3t7Wpvb5ckbd26VYcOHVI8HldOTo4qKyvldDpVXV2txsZG\nJRIJBYNBpaenq6qqSi0tLWpoaJDb7db27dslSXV1ddq/f78SiYQCgYAKCgoWPymwyN7/3/9J9RQA\nfE44LMuyUj2J2TL5svl8npQEXMliP2FtudxvFoqd8tgpi2SvPMvmsjkAAFh+KG8AAAxDeQMAYBjK\nGwAAw1DeAAAYhvIGAMAwlDcAAIahvAEAMAzlDQCAYShvAAAMQ3kDAGAYyhsAAMNQ3gAAGIbyBgDA\nMFf9PG8A9jLbj6Z9/zNfL/ZHkAKYG3beAAAYhvIGAMAwXDYHDDTby98A7ImdNwAAhqG8AQAwDOUN\nAIBhKG8AAAzDE9YAzGg+T5DjteHA4mHnDQCAYShvAAAMQ3kDAGAYyhsAAMNQ3gAAGIbyBgDAMJQ3\nAACGobwBADAM5Q0AgGEobwAADEN5AwBgGMobAADDUN4AABiG8gYAwDCz+kjQd999Vy+++KJ27typ\n8fFxtbS0yOFwKDc3V7W1tXI6nQqFQgqFQnK5XKqpqVFFRYVisZj27dunyclJeTwe1dfXKysrS4OD\ng2pra5PL5VIgENCmTZsWOyeAJTafjxGV+ChRYDZmLO8//vGPeuONN5SRkSFJOnz4sILBoNauXauD\nBw+qs7NTa9as0dGjR9Xc3KypqSk1NDQoEAioo6NDeXl52rx5s44dO6b29nZt27ZNra2t2rFjh7Kz\ns9Xc3KyRkRGtXr160cMCWP747HBgZjNeNs/OztZDDz2U/Hp4eFglJSWSpPLych0/flxDQ0MqLCxU\nWlqavF6vfD6fRkdHNTAwoLKysuSxvb29CofDisfj8vl8cjgcKi0tVW9v7yLFAwDAfmbceVdWVmpi\nYuKSMYfDIUnyeDwKh8MKh8Pyer3J718cj0QiyfGMjIzkmMfjSR6bkZFx2f//Svx+/6yOW6jzFtL7\nqZ4AYGOzuY8vh8eBhWKnLJK98ixVlln9zfvTLha3JEUiEWVmZsrr9SoajV427vF4kuPRaDQ5FolE\nksdGo9FLiv9qxsbG5jpd+f3+eZ0HwBwz3cft9DhgpyySvfIsRpYr/TIw52ebr1q1Sn19fZKk7u5u\nFRcXKz8/X/39/YrFYgqHwzp58qRyc3NVWFiorq6u5LFFRUXyer1yu90aHx+XZVnq6elRcXHxNUQD\nAODzZc477y1btujAgQOKx+PKyclRZWWlnE6nqqur1djYqEQioWAwqPT0dFVVVamlpUUNDQ1yu93a\nvn27JKmurk779+9XIpFQIBBQQUHBggcDAMCuHJZlWamexGyZfNl8vi+bATCzmZ5tvlweBxaCnbJI\n9sqzlJfN57zzBgC74GVpMBXvsAYAgGHYeQMw3kw7aF6qCbth5w0AgGEobwAADEN5AwBgGMobAADD\n8IQ1AFgCvCwNC4nyniPebAUAkGqUNwDMAb/AYzmgvAFgmZruF4XZvGady+32xxPWAAAwDOUNAIBh\nKG8AAAxDeQMAYBjKGwAAw1DeAAAYhpeKAQDm/fp1XpaWGuy8AQAwDDtvALAZ3gXO/th5AwBgGHbe\nAIAl9ekrA7N5u9eL+Pv6f7HzBgDAMOy8AQDzxt/XU4PyBgDYll1fAsdlcwAADMPOGwBghKW8RD+v\nn/V/nQs/kStg5w0AgGEobwAADEN5AwBgGMobAADDUN4AABiG8gYAwDCUNwAAhqG8AQAwDOUNAIBh\nKG8AAAyTsrdHTSQS+tWvfqXR0VGlpaXpnnvukc/nS9V0AAAwRsp23m+99Zampqa0Z88eff/739fz\nzz+fqqkAAGCUlO28BwYGVFZWJklas2aN3nvvvSWfA59DCwAwUcrKOxKJyOv1Jr92Op26cOGCXC7X\nFc/x+/3z+llXPG8JPwEGAGB/8+2puUrZZXOPx6NIJJL82rKsqxY3AAD4RMrKu7CwUN3d3ZKkwcFB\n5eXlpWoqAAAYxWFZlpWKH3zx2eb//Oc/ZVmWfvzjHysnJycVUwEAwCgpK28AADA/vEkLAACGobwB\nADBMyl4qttjs8A5u8Xhczz33nE6dOqWpqSnddddd+sIXvqDm5mbddNNNkqSqqip95StfSfFMZ+eR\nRx6Rx+ORJN14442qqalRS0uLHA6HcnNzVVtbK6fTjN8nX3/9db3++uuSpKmpKZ04cUJPPvmkcWvz\n7rvv6sUXX9TOnTs1Pj4+7XqEQiGFQiG5XC7V1NSooqIi1dO+ok/nOXHihH7961/L6XQqLS1N9fX1\nuv7663Xo0CENDAwkb4s//elPL3nZ6nLx6SwjIyPT3rZMXZuf//znOnv2rCTp1KlTKigo0AMPPLDs\n12a6x+Sbb745Nfcby6befPNN69lnn7Usy7Leeecda+/evSme0dy9+uqr1qFDhyzLsqwPP/zQuuee\ne6xQKGS98sorqZ3YPJw/f956+OGHLxlrbm623n77bcuyLOvAgQPW3//+91RM7Zq1trZaf/nLX4xb\nm5dfftl68MEHrccee8yyrOnX4z//+Y/14IMPWrFYzPr444+T/70cfTbPz372M2tkZMSyLMvq6Oiw\n2traLMuyrCeeeMI6d+5cqqY5K5/NMt1ty+S1uejDDz+0HnroIevMmTOWZS3/tZnuMTlV9xsztjnz\nsBzewe1a3Xbbbfrud78r6b+vgx8eHlZXV5caGxv13HPPXfJa+eVsdHRU58+f15NPPqmmpiYNDg5q\neHhYJSUlkqTy8nIdP348xbOcu/fee0//+te/9LWvfc24tcnOztZDDz2U/Hq69RgaGlJhYaHS0tLk\n9Xrl8/k0Ojqaqilf1WfzPPDAA1q1apUk6cKFC0pLS1MikdD4+LgOHjyohoYGvfrqqyma7dVNtzaf\nvW2ZvDYX/e53v1N1dbVuuOEGI9bmSo/Jqbjf2Pay+XzewW25ycjIkPRJlmeeeUbBYFBTU1O64447\ndMstt+gPf/iDfv/732vLli0pnunMVqxYoTvvvFN33HGHPvjgAz311FOSJIfDIemTN+0Jh8OpnOK8\nHDlyRN/5znckSfn5+UatTWVlpSYmJi4Z++x6hMPhS+5Hy3mdPpvnhhtukCS98847+vOf/6ympiad\nP39e3/zmN/Xtb39biURCTU1N+vKXv6wvfelLqZr2tD6bZbrb1qpVq4xdG0k6d+6c3n77bW3dulWS\njFib6R6TX3jhhZTcb2y787bLO7idPn1aTU1N+upXv6rbb79d69at0y233CJJWrdunU6cOJHaCc7S\nTTfdpPXr18vhcMjv9+u6665L/s1L+uTOkJmZmcIZzt3HH3+ssbEx3XrrrZJk7NpcdPEBSPrveni9\nXkWj0cvGTfG3v/1Nra2tevTRR5WVlaUVK1boW9/6llasWCGPx6Nbb7112e5WP22625bpa/Pmm2/q\n9ttvTz7PxZS1+exjcqruN7Ytbzu8g9vZs2e1Z88e3X333dqwYYMkac+ePRoaGpIk9fb2Ju/Qy91r\nr72W/OS4M2fOKBKJqLS0VH19fZKk7u5uFRcXp3KKc9bf358sbsnctblo1apVl61Hfn6++vv7FYvF\nFA6HdfLkSeXm5qZ4prPzxhtv6E9/+pN27typ7OxsSdLY2JgaGhqUSCQUj8c1MDCg1atXp3imM5vu\ntmXy2kif5Lj4p03JjLWZ7jE5Vfcb2142X7dunY4fP64nnngi+Q5upjly5Ig++ugjtbe3q729XZK0\nZcsWHT58WC6XS9dff71++MMfpniWs7Nhwwa1tLSooaFBDodD9957r1auXKkDBw4oHo8rJydHlZWV\nqZ7mnIyNjSVLQZJ+8IMf6NChQ8atzUVbtmy5bD2cTqeqq6vV2NioRCKhYDCo9PT0VE91RolEQocO\nHdIXv/hFPf3005KkkpISbd68WevXr9fjjz8ul8ul9evXG1F40922vF6vkWtz0WfvPzfffPOyX5vp\nHpO3bt2qQ4cOLfn9hndYAwDAMLa9bA4AgF1R3gAAGIbyBgDAMJQ3AACGobwBADAM5Q0AgGEobwAA\nDEN5AwBgmP8HmKbi2+udbhcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12783b210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's put the lengths into a histogram to see the distributions of the lengths\n",
    "plt.hist([x for x in doc_lengths if x <= 200], bins=30)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cap the maximum length of the reviews to 200 words, as that will give us a good sense of the overall data without having to zero pad the short reviews to match really long reviews.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#zero pad the shorter texts\n",
    "max_length = 200\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872496\n"
     ]
    }
   ],
   "source": [
    "#find the vocab size from tokenizer\n",
    "vocab_size = t.word_counts[max(t.word_counts,key=t.word_counts.get)]\n",
    "print vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "now that we have the data, we can split this into train/test and create a CNN using Keras. <br>\n",
    "The CNN will be layered as follows:<br>\n",
    "\n",
    "- The embedding layer, which is the input layer consisting of the embedded word vectors.\n",
    "- A Flatten layer, to make the 2-dimensional input into a 1-dimensional output.\n",
    "- A Dense layer, which will create a fully connected layer with 1 node for the output.\n",
    "- activation function is sigmoid, because this will give us a binary output, either positive review or non-positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data\n",
    "train_size = 0.7\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    padded_docs, labels, test_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 8)            14979968  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1601      \n",
      "=================================================================\n",
      "Total params: 14,981,569\n",
      "Trainable params: 14,981,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# #create the CNN\n",
    "# vect_dimension = 8\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# #model compilation. add loss function\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# # check model\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 200, 32)           59919872  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 200, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 3201      \n",
      "=================================================================\n",
      "Total params: 59,926,177\n",
      "Trainable params: 59,926,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create the CNN\n",
    "vect_dimension = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "model.add(Conv1D(32, kernel_size=3, padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#model compilation. add loss function\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# check model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 4412s - loss: 0.3039 - acc: 0.8735  \n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 4079s - loss: 0.1896 - acc: 0.9280  \n",
      "169984/170536 [============================>.] - ETA: 0sAccuracy: 96.052446\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397312/397918 [============================>.] - ETA: 0sAccuracy: 91.465076\n"
     ]
    }
   ],
   "source": [
    "#put on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc after epoch 1 \n",
    "\n",
    "vect dim 8, filter_size 8: 8623\n",
    "8, 32: 88, 91, 94, 91\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Here, we will try different parameters to tune the CNN and check the performance. \n",
    "The first cell will focus on how many dimensions to use and the second cell will focus on different optimizers/loss functions and the result will be plotted in a heatmap to visualize the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim:  2\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 280s - loss: 0.3267 - acc: 0.8595   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 280s - loss: 0.2442 - acc: 0.9024   \n",
      "168416/170535 [============================>.] - ETA: 0sTraining Accuracy: 91.319670\n",
      "396928/397917 [============================>.] - ETA: 0sTest Accuracy: 89.581244\n",
      "\n",
      "\n",
      "dim:  4\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 547s - loss: 0.3149 - acc: 0.8657   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 545s - loss: 0.2360 - acc: 0.9057   \n",
      "169280/170535 [============================>.] - ETA: 0sTraining Accuracy: 91.904888\n",
      "396128/397917 [============================>.] - ETA: 0sTest Accuracy: 89.849642\n",
      "\n",
      "\n",
      "dim:  6\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 799s - loss: 0.3077 - acc: 0.8701   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 801s - loss: 0.2301 - acc: 0.9088   \n",
      "170336/170535 [============================>.] - ETA: 0sTraining Accuracy: 92.755153\n",
      "395520/397917 [============================>.] - ETA: 0sTest Accuracy: 90.160511\n",
      "\n",
      "\n",
      "dim:  8\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 1063s - loss: 0.3040 - acc: 0.8717  \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 1087s - loss: 0.2265 - acc: 0.9102  - ETA: 9s - loss: - ETA: 5s - \n",
      "169344/170535 [============================>.] - ETA: 0sTraining Accuracy: 92.724074\n",
      "397312/397917 [============================>.] - ETA: 0sTest Accuracy: 90.017265\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create multiple CNNs with different hyperparameters\n",
    "\n",
    "vect_dimensions = [2,4,6,8]\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "for dim in vect_dimensions:\n",
    "    print 'dim: ', dim\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, dim, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "    #put on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    models_accuracy[model]['training_error'] = train_accuracy\n",
    "    models_accuracy[model]['test_error']=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the mdoels which was just created\n",
    "post_name = 1\n",
    "for k,v in models_accuracy.iteritems():\n",
    "    out_name = 'keras_model_'+str(post_name)\n",
    "    k.save(out_name)\n",
    "    post_name+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling.ipynb         keras_model_1          keras_model_4\r\n",
      "Proposal.md            keras_model_2          my_model.h5\r\n",
      "data_preparation.ipynb keras_model_3\r\n"
     ]
    }
   ],
   "source": [
    "#chcek for the model files\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer:  adam\n",
      "loss:  mean_squared_error \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 243s - loss: 0.1019 - acc: 0.8621   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 236s - loss: 0.0722 - acc: 0.9052   \n",
      "167328/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 92.103087\n",
      "397120/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 90.006207\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  adam\n",
      "loss:  cosine_proximity \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 231s - loss: -0.7797 - acc: 0.7791   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 231s - loss: -0.7797 - acc: 0.7797   \n",
      "169632/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 77.970505\n",
      "396224/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 78.108751\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  adam\n",
      "loss:  binary_crossentropy \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 231s - loss: 0.3266 - acc: 0.8599   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 233s - loss: 0.2442 - acc: 0.9016   \n",
      "170016/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 91.474477\n",
      "397792/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 89.680009\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  sgd\n",
      "loss:  mean_squared_error \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 122s - loss: 0.1733 - acc: 0.7781   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 122s - loss: 0.1706 - acc: 0.7797   \n",
      "169920/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 77.970505\n",
      "395680/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 78.108751\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  sgd\n",
      "loss:  cosine_proximity \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 124s - loss: -0.7797 - acc: 0.7108   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 123s - loss: -0.7797 - acc: 0.7108   \n",
      "169696/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 71.080423\n",
      "394880/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 70.969071\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  sgd\n",
      "loss:  binary_crossentropy \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 128s - loss: 0.5254 - acc: 0.7791   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 123s - loss: 0.5170 - acc: 0.7797   \n",
      "170496/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 77.970505\n",
      "396960/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 78.108751\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adagrad\n",
      "loss:  mean_squared_error \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 181s - loss: 0.1206 - acc: 0.8302   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 182s - loss: 0.0934 - acc: 0.8743   \n",
      "168544/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 88.134987\n",
      "396704/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 87.423005\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adagrad\n",
      "loss:  cosine_proximity \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 187s - loss: -0.7797 - acc: 0.7796   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 182s - loss: -0.7797 - acc: 0.7797   \n",
      "170368/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 77.970505\n",
      "397824/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 78.108751\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adagrad\n",
      "loss:  binary_crossentropy \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 181s - loss: 0.3811 - acc: 0.8254   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 186s - loss: 0.3055 - acc: 0.8689   - ETA: 1s - loss:\n",
      "169760/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 87.827719\n",
      "396352/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 87.222461\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adadelta\n",
      "loss:  mean_squared_error \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 311s - loss: 0.1478 - acc: 0.7943   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 314s - loss: 0.1089 - acc: 0.8479   \n",
      "169600/170535 [============================>.] - ETA: 0s ETA\n",
      " Training Accuracy: 86.083795\n",
      "395072/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 85.772661\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adadelta\n",
      "loss:  cosine_proximity \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 309s - loss: -0.7797 - acc: 0.5201   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 307s - loss: -0.7797 - acc: 0.5224   \n",
      "167936/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 52.356408\n",
      "395616/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 52.221192\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adadelta\n",
      "loss:  binary_crossentropy \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 302s - loss: 0.4450 - acc: 0.7977   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 302s - loss: 0.3384 - acc: 0.8505   \n",
      "168768/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 86.259712\n",
      "397600/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 85.881226\n",
      "\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optimizers = ['adam', 'sgd', 'Adagrad', 'Adadelta']\n",
    "losses = ['mean_squared_error', 'cosine_proximity', 'binary_crossentropy']\n",
    "metrics=['acc']\n",
    "\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for loss in losses:\n",
    "        print 'optimizer: ', optimizer\n",
    "        print 'loss: ', loss, '\\n'\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, 2, input_length=max_length))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        #model compilation. add loss function\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "        # fit the model\n",
    "        model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "        # evaluate the model\n",
    "        train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "        print('\\n Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "        #put on test set\n",
    "        test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "        print('\\n Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        #models_accuracy[model]['training_error'] = train_accuracy\n",
    "        #models_accuracy[model]['test_error']=test_accuracy\n",
    "        models_accuracy[optimizer][loss] = defaultdict(dict)\n",
    "        models_accuracy[optimizer][loss]['train_acc']=train_accuracy\n",
    "        models_accuracy[optimizer][loss]['test_acc']=test_accuracy\n",
    "\n",
    "        \n",
    "        print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with open('hyper_parameter_tuning.pickle', 'wb') as handle:\n",
    "    pickle.dump(models_accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('hyper_parameter_tuning.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print models_accuracy == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
