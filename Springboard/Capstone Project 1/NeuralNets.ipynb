{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Keras on Amazon's fine food review part 2\n",
    "\n",
    "## Using Convolutional Neural Networks\n",
    "\n",
    "This notebook will create a convolutional neural network (CNN) off of the fine food review to suggest if the review is a positive (4 or 5 star rating) or a non-positive (1, 2, or 3 star rating).  What we aim here is to use the word2vec embedding implementation in Keras to convert the text into word vectors, and train a CNN with it which will predict, given a review text whethe or not the rating is positive or non-positive.  The hyperparameter tuning of the CNN will be based off of the article \"A Sensitivity Analysis of (and Practitioners' guide to) Convolutional Neural Networks for Sentence Classification.\" https://arxiv.org/pdf/1510.03820.pdf\n",
    "\n",
    "The steps taken during this notebook are the follows:<br>\n",
    "- Read in the data and convert into a pandas dataframe.\n",
    "- The text will be turned into vectors using keras implementation of word2vec\n",
    "- distribution of the vectors will be analyzed to check for outliers and discard them as necessary.\n",
    "- short vectors will be 0-padded to match in length to the longer vectors.\n",
    "- CNNs will be trained using different parameters and performance compared amongst the other CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data importing/wrangling\n",
    "import pandas as pd\n",
    "\n",
    "#text processing \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#CNN modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, Conv1D, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "#to suppress the epoch training outputs\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "#keras functionsl api\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "#plotting CNN\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, Activation\n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "#data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "#only required first time\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import scipy.sparse\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "input_location = '/Users/momori/data/reviews_processed.csv'\n",
    "#for aws\n",
    "#input_location='/home/ubuntu/data/reviews_processed.csv'\n",
    "\n",
    "model_output = '/Users/momori/data/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(input_location)\n",
    "\n",
    "#smaller sample to test code\n",
    "#data = data.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create documents and labels to train the model later\n",
    "docs = data['Text']\n",
    "labels = data['positive_review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embedding\n",
    "\n",
    "Here the text documents will be massaged into numerical vectors which will be fed into the keras embedding layer.  We will first use a 1-hot vector encoding of the documents to turn them into vectors, it works as follows:\n",
    "1. Count the number of vocabularies in the set of documents and creates a list such that each index in the list corresponds to a word.\n",
    "2. For each document, encode the sentence into numbers by doing a lookup onto the list and getting an interger value.\n",
    "\n",
    "For example, let's say the set of vocabulary consists of 'i', 'eat', 'apples', 'oranges'. Since there are 4 words, we create a list of size 4 and include the words as the elements:\n",
    "['i', 'eat', 'apples', 'oranges'].\n",
    "\n",
    "Now let's say we have two sentences, 'I eat apples' and 'I eat oranges.' Then these would be encoded as [1,2,3] and [1,2,4] respectively.  \n",
    "\n",
    "Once the encodings are done, we will analyze the lengths of the documents to see if there are any outliers such as very long reviews.  To analyze the vectors in the neural network , all the encoded documents must be the same length, hence the shorter reviews will need to be 0-padded (adding entries of 0 at the end to match the length to the longest review). By checking the percentiles of the distribution, we may be able to remove some of the longer reviews to avoid 0-padding too much.\n",
    "\n",
    "After checking the percentiles, it was deemed that we can cap the maximum length of the reviews to 200 words, as that will give us a good sense of the overall data without having to zero pad the short reviews to match really long reviews, and also is in the ~93rd percentile.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 17, 125, 319, 7, 1, 4940, 523, 103, 52, 204, 3, 17, 117, 28, 41, 5, 30, 7, 29, 183, 1, 38, 629, 48, 26, 4, 2636, 58, 4, 1183, 448, 3, 6, 619, 99, 13, 5266, 8, 1777, 3, 94, 8695, 9, 38, 99, 58, 140]\n"
     ]
    }
   ],
   "source": [
    "#1-hot vector encoding\n",
    "\n",
    "#use the tokenizer API provided by Keras to turn documents into sequences\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "#encode the documents into integers\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a list of lengths of each of the encoded_docs to find out the longest length, \n",
    "#and see if any 0 padding is required\n",
    "doc_lengths = [len(doc) for doc in encoded_docs]\n",
    "max_doc_length = max(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  percentile of document length: 57.0\n",
      "60  percentile of document length: 70.0\n",
      "70  percentile of document length: 88.0\n",
      "80  percentile of document length: 114.0\n",
      "90  percentile of document length: 164.0\n",
      "99  percentile of document length: 398.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMIIgA4ARBBkAjPgf6qs6AGZ2iOYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cc3be10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's put the lengths into a boxplot to see the distributions of the lengths\n",
    "np_array = np.array(doc_lengths)\n",
    "\n",
    "for i in [50,60,70,80,90,99]:\n",
    "    print i, \" percentile of document length:\", np.percentile(np_array,i)\n",
    "    \n",
    "ax = sns.boxplot(x=doc_lengths, showfliers=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1af7463f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.411967526\n"
     ]
    }
   ],
   "source": [
    "#let's put the lengths into a histogram to see the distributions of the lengths\n",
    "plt.hist([x for x in doc_lengths if x <= 200], bins=30)\n",
    "plt.show()\n",
    "print stats.percentileofscore(np_array, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#zero pad the shorter texts\n",
    "max_length = 200\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872496\n"
     ]
    }
   ],
   "source": [
    "#find the vocab size from tokenizer\n",
    "vocab_size = t.word_counts[max(t.word_counts,key=t.word_counts.get)]\n",
    "print vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "now that we have the data, we can split this into train/test and create a CNN using Keras.\n",
    "The CNN will be layered as follows:\n",
    "\n",
    "1. The input layer, which will just be defined along with the input shape of the data.\n",
    "2. The embedding layer. This will take as input the numerical vector encoding which was established above.  This is the first hidden layer of the network and will turn the original encoding into encoded vectors in the 'output_dimension' specified\n",
    "3. A convolutional layer which is specified with a 'filter size.'  It will look at the embedded vector by sliding the filter along the list, applying a convolution and saving the output.  For example, consider a case where the input length of this layer is a list of 50 elements and the filter size is 3.  Then, the output will be a 50-3+1 = 48 different convolutions.\n",
    "4. A pooling layer, which looks for the most significant feature in each of the convolution. In our case, we are using max-pooling, so it will look for the largest value in the convoluted results.\n",
    "5. A flatten layer, to change the dimensionality of the data into a 1-D array.\n",
    "6. A dense layer (which is the output layer), which will connect all outputs from the flatten layer, apply an activation function and give us the result.\n",
    "    \n",
    "In addition, the below cells will make use of the article \"A Sensitivity Analysis of(and Pracitioners' Guide to) Convolustional Neural Networks for Sentence Classification\" mentioned at the top of this notebook.  \n",
    "\n",
    "The authors of the paper have tuned the various hyperparameters used in a CNN and provided the resulting best practices to find the best ones.  We will investigate some of the hyperparameters mentioned in the paper, which are filter region size, number of feature maps, and activation functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data\n",
    "test_size = 0.3\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    padded_docs, labels, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #let's use smaller sample size\n",
    "# train_size = 0.7\n",
    "# samp_size = 0.1\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     padded_docs[:int(len(padded_docs)*samp_size)], labels[:int(len(padded_docs)*samp_size)]\\\n",
    "#     , test_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397917, 170537)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for convolutional neural network. Used to simplify the many \n",
    "#for loops required to go through hyper parameter tuning\n",
    "\n",
    "class cnn(object):\n",
    "    def __init__(self, vocab_size, vect_dimensions, max_doc_length,\\\n",
    "                 training_set_length,\n",
    "                 kernel_sizes=None, activation='relu',\\\n",
    "                num_feature_maps = 1):\n",
    "        if kernel_sizes is None:#or type(a) is not list:\n",
    "            self._kernel_sizes = [5]\n",
    "        elif type(kernel_sizes) is not list:\n",
    "            self._kernel_sizes = [kernel_sizes]\n",
    "        else:\n",
    "            self._kernel_sizes = kernel_sizes\n",
    "        self._vocab_size = vocab_size\n",
    "        self._vect_dimensions = vect_dimensions\n",
    "        self._max_doc_length = max_doc_length\n",
    "        self._training_set_length = training_set_length\n",
    "        self._activation = activation\n",
    "        self._num_feature_maps = num_feature_maps\n",
    "        self._model = None\n",
    "        \n",
    "    @property\n",
    "    def kernel_sizes(self):\n",
    "        return self._kernel_sizes\n",
    "    \n",
    "    @kernel_sizes.setter\n",
    "    def kernel_sizes(self, value):\n",
    "        self._kernel_sizes = value\n",
    "        \n",
    "    @property \n",
    "    def activation(self):\n",
    "        return self._activation\n",
    "    \n",
    "    @activation.setter\n",
    "    def activation(self, value):\n",
    "        if value not in ['relu','sigmoid','tanh']:\n",
    "            print 'bad activation value. defaulting to relu'\n",
    "            self._activation = 'relu'\n",
    "        else:\n",
    "            self._activation = value\n",
    "            \n",
    "    @property\n",
    "    def num_feature_maps(self):\n",
    "        return self._num_feature_maps\n",
    "    \n",
    "    @num_feature_maps.setter\n",
    "    def num_feature_maps(self, value):\n",
    "        self._num_feature_maps = value\n",
    "    \n",
    "    def list_params(self):\n",
    "        print 'filters:',self._filters\n",
    "        print 'vocab_size:', self._vocab_size\n",
    "        print 'vect_dimensions:', self._vect_dimensions\n",
    "        print 'max_doc_length:', self._max_doc_length\n",
    "        print 'training_set_length:', self._training_set_length\n",
    "        print 'kernel_size:', self._kernel_size\n",
    "        print 'activation:', self._activation\n",
    "        print 'num_feature_maps:', self._num_feature_maps\n",
    "        \n",
    "    def create(self):\n",
    "        #create the input layer\n",
    "        visible = Input(shape=(self._max_doc_length,))\n",
    "    \n",
    "        #create the embedding layer\n",
    "        x = Embedding(self._vocab_size,\n",
    "                      self._vect_dimensions, \n",
    "                      input_length=self._max_doc_length)(visible)\n",
    "        \n",
    "        #check length of filters. if 1 then we create a basic neural network with\n",
    "        #one intput/output per node.\n",
    "        #if more than one filter, than we branch the embedding layer into multiple layers\n",
    "        num_filters = len(self._kernel_sizes)\n",
    "        if num_filters > 1:\n",
    "            #create a list of convolutions/pools/flatten layers\n",
    "            convolutions = [None] * num_filters\n",
    "            pools = [None] * num_filters\n",
    "            flattens = [None] * num_filters\n",
    "            \n",
    "            #for each filter in self._filters, create a filter\n",
    "            for index in range(num_filters):\n",
    "                #the article suggests to use a maxpooling where the pooling size is equivalent\n",
    "                #to the size of the convoluted layer. \n",
    "                convolutions[index] = Conv1D(self._num_feature_maps,\n",
    "                                            kernel_size=(self._kernel_sizes[index],),\n",
    "                                            activation=self._activation,\n",
    "                                            input_shape=(self._training_set_length,))(x)\n",
    "                pools[index] = MaxPooling1D(pool_size = self._max_doc_length - \n",
    "                                            self._kernel_sizes[index] + 1)(convolutions[index])\n",
    "                flattens[index] = Flatten()(pools[index])\n",
    "        \n",
    "            #after the filters are created, merge them\n",
    "            merge = concatenate(flattens)\n",
    "            \n",
    "        else: #only 1 filter case\n",
    "            conv1 = Conv1D(self._num_feature_maps,\n",
    "                          kernel_size = (self._kernel_sizes[0],),\n",
    "                          activation=self._activation)(x)\n",
    "            pool1 = MaxPooling1D(pool_size = self._max_doc_length - self._kernel_sizes[0] + 1)(conv1)\n",
    "            merge = Flatten()(pool1)\n",
    "            \n",
    "        #after the filter layers are merged, add dense layers\n",
    "        #hidden1 = Dense(10, activation=self._activation)(merge)\n",
    "        output = Dense(1, activation='sigmoid')(merge)\n",
    "        self._model = Model(inputs = visible, outputs = output)\n",
    "\n",
    "                \n",
    "    def plot_model(self, name):\n",
    "        file_name = '/users/momori/data/models/'+str(name)+'.png'\n",
    "        print 'saving file as', file_name\n",
    "        plot_model(self._model, to_file=file_name)\n",
    "        !open $file_name\n",
    "    \n",
    "    def summarize_model(self):\n",
    "        print self._model.summary()\n",
    "    \n",
    "    def compile_fit(self, x_train, y_train, epochs=2, verbose=1):\n",
    "        #to output the training logs into a file\n",
    "        csv_logger = CSVLogger('training_log.csv', append=True, separator=';')\n",
    "\n",
    "        self._model.compile(optimizer = 'adam',\\\n",
    "                                          loss='binary_crossentropy',\n",
    "                                          metrics=['acc'])\n",
    "        self._model.fit(x_train, y_train, epochs=epochs,\n",
    "                                verbose=verbose, callbacks=[csv_logger])\n",
    "    \n",
    "    def evaluate(self, x_data, y_data, verbose=1):\n",
    "        loss, accuracy = self._model.evaluate(x_data, y_data, verbose=verbose)\n",
    "        return accuracy*100\n",
    "    \n",
    "    def save(self, name):\n",
    "        file_name = '/users/momori/data/models/'+str(name)\n",
    "        print 'saving model as:', file_name\n",
    "        self._model.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single kernel size analysis\n",
    "\n",
    "In the sensitivy analysis paper, the authors find that the best single filter region size is commonly found between 1 and 10. And once the best single size is found, it is recommended to try a few different combinations of different regions sizes around the best one.  The below cell will first find the single best region size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel size is: [1]\n",
      "Epoch 1/2\n",
      "397917/397917 [==============================] - 1105s 3ms/step - loss: 0.4749 - acc: 0.7960\n",
      "Epoch 2/2\n",
      "397917/397917 [==============================] - 2017s 5ms/step - loss: 0.4174 - acc: 0.8138\n",
      "training accuracy: 81.5926436921\n",
      "testing accuracy: 81.5025478342\n",
      "saving model as: /users/momori/data/models/model_kernel_size1\n",
      "kernel size is: [2]\n",
      "Epoch 1/2\n",
      "397917/397917 [==============================] - 1157s 3ms/step - loss: 0.4360 - acc: 0.8145\n",
      "Epoch 2/2\n",
      "397917/397917 [==============================] - ETA: 0s - loss: 0.3772 - acc: 0.8347- ETA: 1s - loss: 0.3772 - 1165s 3ms/step - loss: 0.3772 - acc: 0.8347\n",
      "training accuracy: 84.0793431796\n",
      "testing accuracy: 83.6780288149\n",
      "saving model as: /users/momori/data/models/model_kernel_size2\n",
      "kernel size is: [3]\n",
      "Epoch 1/2\n",
      "397917/397917 [==============================] - 1063s 3ms/step - loss: 0.4499 - acc: 0.8036\n",
      "Epoch 2/2\n",
      "194208/397917 [=============>................] - ETA: 9:00 - loss: 0.3860 - acc: 0.8357"
     ]
    }
   ],
   "source": [
    "#let's use the above neural network to find the optimal window size\n",
    "k_size = [i for i in range(1,12)]\n",
    "models = []\n",
    "for k in k_size:\n",
    "    #create the CNN\n",
    "    vect_dimension = 4 \n",
    "    nn = cnn(vocab_size, vect_dimension, max_length, len(x_train),\n",
    "            kernel_sizes = k)\n",
    "    print 'kernel size is:', nn.kernel_sizes\n",
    "    nn.create()\n",
    "    nn.compile_fit(x_train, y_train, epochs=2, verbose=1)\n",
    "    print 'training accuracy:', nn.evaluate(x_train, y_train, verbose=0)\n",
    "    print 'testing accuracy:',nn.evaluate(x_test, y_test, verbose=0)\n",
    "    nn.save('model_kernel_size'+str(k))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above cell, we've tried window size of 1-11. Here are the results:\n",
    "\n",
    "| window size | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      1      |         asdfd       |      80.1        |\n",
    "|      2      |         86.22       |       82.43      |\n",
    "|      3      |        86.65        |       82.70      |\n",
    "|      4      |       87.31         |       83.15      |\n",
    "|      5      |      86.13          |       82.41      |\n",
    "|      6      |       89.24         |       83.82      |\n",
    "|      7      |        86.86        |       83.42      |\n",
    "|      8      |        88.65        |       83.96      |\n",
    "|      9      |        88.96        |       83.83      |\n",
    "|      10     |        88.22        |       83.74      |\n",
    "|      11     |        87.41        |        83.25     |\n",
    "\n",
    "We see the maximum accuracy for both training and testing when the window_size is (). Hence we will try different number and combinations of filter size around ()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple kernel size analysis\n",
    "\n",
    "From the above cell, we've found the best single resion size to be ******\n",
    "Below, we will test different combinations of multiple region sizes which are close to the best fit region size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##create the models with different filter sizes\n",
    "vect_dimension = 4 \n",
    "\n",
    "#update below accordingly to the above finding\n",
    "g_filter = 8\n",
    "\n",
    "filters = [[g_filter],[g_filter,g_filter],[g_filter,g_filter,g_filter],\n",
    "           [g_filter-1, g_filter],[g_filter, g_filter+1],\n",
    "           [g_filter-1, g_filter, g_filter+1]]\n",
    "\n",
    "\n",
    "filters = [[g_filter,g_filter],[g_filter,g_filter,g_filter],\n",
    "           [g_filter-1, g_filter],[g_filter, g_filter+1],\n",
    "           [g_filter-1, g_filter, g_filter+1]]\n",
    "\n",
    "for f in filters:\n",
    "    #create the CNN\n",
    "    nn = cnn(vocab_size, vect_dimension, max_length, len(x_train),\n",
    "            kernel_sizes = f)\n",
    "    print 'kernel size(s) are:', f\n",
    "    nn.create()\n",
    "    nn.compile_fit(x_train, y_train, epochs=2, verbose=0)\n",
    "    print 'training accuracy:', nn.evaluate(x_train, y_train, verbose=0)\n",
    "    print 'testing accuracy:',nn.evaluate(x_test, y_test, verbose=0)\n",
    "    nn.save('model_kernel_sizes'+str(k))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different accuracies achieved by the above filters are below:\n",
    "\n",
    "| filters used | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      6     |         91.87       |      88.05        |\n",
    "|      6,6      |         91.87       |       88.05      |\n",
    "|      6,6,6      |        88.33        |       86.69      |\n",
    "|      5,6      |       88.33         |       86.69     |\n",
    "|      6,7      |      88.33          |       86.69      |\n",
    "|      5,6,7    |       91.87         |       88.05      |\n",
    "\n",
    "Due to the low number of samples used in training/testing, the neural network may have achieved its maximum fit with the above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis: Feature maps\n",
    "\n",
    "WIP:\n",
    "According to the article, feature maps should range between 100-600. We will spot check some of them and check the accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = [10,50,100,200,400,600,700,800,900,1000]\n",
    "for feature in feature_maps:\n",
    "    #create the CNN\n",
    "    nn = cnn(vocab_size, vect_dimension, max_length, len(x_train),\n",
    "            num_feature_maps = feature)\n",
    "    print 'number of feature maps are:', nn.num_feature_maps\n",
    "    nn.create()\n",
    "    nn.compile_fit(x_train, y_train, epochs=2, verbose=0)\n",
    "    print 'training accuracy:', nn.evaluate(x_train, y_train, verbose=0)\n",
    "    print 'testing accuracy:',nn.evaluate(x_test, y_test, verbose=0)\n",
    "    nn.save('model_num_feature_maps'+str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP:<br>\n",
    "we see from the below table that we achieve the highest testing accuracy with 800 feature maps per convolution.\n",
    "\n",
    "| number of features | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      10    |         92.72      |      88.15        |\n",
    "|      50     |         94.10     |       89.03      |\n",
    "|      100      |        94.21        |       88.91      |\n",
    "|      200      |       94.33         |       89.23    |\n",
    "|      400      |      94.75         |       89.41|\n",
    "|      600    |       94.68         |       89.39      |\n",
    "|      700    |       94.41         |       88.82      |\n",
    "|      800    |       95.05         |       89.55      |\n",
    "|      900    |       94.57         |       89.37      |\n",
    "|      1000    |       92.93         |      87.62      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activation functions\n",
    "\n",
    "According to the article, RELU and TANH is the best suited functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's use the above neural network to find the optimal window size\n",
    "activation_functions = ['relu','tanh', 'sigmoid']\n",
    "\n",
    "for func in activation_functions:\n",
    "    #create the CNN\n",
    "    nn = cnn(vocab_size, vect_dimension, max_length, len(x_train),\n",
    "            activation = func)\n",
    "    print 'activation is:', nn.activation\n",
    "    nn.create()\n",
    "    nn.compile_fit(x_train, y_train, epochs=2, verbose=0)\n",
    "    print 'training accuracy:', nn.evaluate(x_train, y_train, verbose=0)\n",
    "    print 'testing accuracy:',nn.evaluate(x_test, y_test, verbose=0)\n",
    "    nn.save('model_activation_functions_'+str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| activation function | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      relu    |         93.96      |      88.87        |\n",
    "|      tanh     |         94.50     |       87.66      |\n",
    "|      sigmoid     |         94.50     |       87.66      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all the results together\n",
    "\n",
    "From the above, we've determined some of the best hyperparameters to use for this cnn.  Here we will implement and test the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 93.7696074107\n",
      "testing accuracy: 88.1282669883\n",
      "saving model as: /users/momori/data/models/out_final_model\n",
      "saving file as /users/momori/data/models/final_model.png\n"
     ]
    }
   ],
   "source": [
    "activation_fun = 'relu'\n",
    "num_feature_maps = 400\n",
    "kernel_sizes = [8, 8, 8]\n",
    "\n",
    "nn = cnn(vocab_size, vect_dimension, max_length, len(x_train),\n",
    "        activation = activation_fun, num_feature_maps = num_feature_maps,\n",
    "        kernel_sizes = kernel_sizes)\n",
    "nn.create()\n",
    "nn.compile_fit(x_train, y_train, epochs=2, verbose=0)\n",
    "print 'training accuracy:', nn.evaluate(x_train, y_train, verbose=0)\n",
    "print 'testing accuracy:',nn.evaluate(x_test, y_test, verbose=0)\n",
    "nn.save('out_final_model')\n",
    "\n",
    "nn.plot_model('final_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sensitivity analysis: window_size\n",
    "\n",
    "From the above cell, we've tried window size of 1-11. Here are the results:\n",
    "\n",
    "| window size | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      1      |         82.98       |      80.1        |\n",
    "|      2      |         86.22       |       82.43      |\n",
    "|      3      |        86.65        |       82.70      |\n",
    "|      4      |       87.31         |       83.15      |\n",
    "|      5      |      86.13          |       82.41      |\n",
    "|      6      |       89.24         |       83.82      |\n",
    "|      7      |        86.86        |       83.42      |\n",
    "|      8      |        88.65        |       83.96      |\n",
    "|      9      |        88.96        |       83.83      |\n",
    "|      10     |        88.22        |       83.74      |\n",
    "|      11     |        87.41        |        83.25     |\n",
    "\n",
    "We see the maximum accuracy for both training and testing when the window_size is 6. Hence we will try different number and combinations of filter size around 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Embedding(vocab_size, vect_dimension, input_length=max_length)\n",
    "# visible = Input(shape=(len(padded_docs[0]),))\n",
    "\n",
    "# #embedding\n",
    "# x = Embedding(vocab_size, vect_dimension, input_length=max_length)(visible)\n",
    "\n",
    "# #first feature extractor\n",
    "# conv1 = Conv1D(2, kernel_size = (6,), activation='relu'\\\n",
    "#                      ,input_shape=(len(x_train),))(x)\n",
    "\n",
    "# pool1 = MaxPooling1D(pool_size=4)(conv1)\n",
    "# flat1 = Flatten()(pool1)\n",
    "\n",
    "# #second feature extractor\n",
    "# conv2 = Conv1D(2, kernel_size=(6,), activation='relu'\\\n",
    "#               ,input_shape=(len(x_train),))(x)\n",
    "# pool2 = MaxPooling1D(pool_size=4)(conv2)\n",
    "# flat2 = Flatten()(pool2)\n",
    "\n",
    "# #concatenate the different feature extractors\n",
    "# merge = concatenate([flat1, flat2])\n",
    "\n",
    "# #add a dense layer\n",
    "# hidden1 = Dense(10, activation='relu')(merge)\n",
    "\n",
    "# #output layer\n",
    "# output = Dense(1, activation='sigmoid')(hidden1)\n",
    "# model=Model(inputs = visible, outputs = output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "34107/34107 [==============================] - 86s 3ms/step - loss: 0.4335 - acc: 0.8069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9b30da90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# model.fit(x_train,y_train)\n",
    "# loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79583/79583 [==============================] - 5s 61us/step\n",
      "Accuracy: 85.221718\n"
     ]
    }
   ],
   "source": [
    "# loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, <tf.Tensor 'flatten_6/Reshape:0' shape=(?, ?) dtype=float32>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,) 200\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_36 (Embedding)     (None, 200, 4)            7489984   \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 195, 2)            50        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 48, 2)             0         \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 10)                970       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 7,491,015\n",
      "Trainable params: 7,491,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##create the models with different filter sizes\n",
    "vect_dimension = 4 \n",
    "\n",
    "filters = [[6],[6,6],[6,6,6],[5,6],[6,7],[5,6,7]]\n",
    "model_dictionary = {}\n",
    "for f in filters:\n",
    "    visible = Input(shape=(len(padded_docs[0]),))\n",
    "    print (len(padded_docs[0]),), max_length\n",
    "\n",
    "    #embedding\n",
    "    x = Embedding(vocab_size, vect_dimension, input_length=max_length)(visible)\n",
    "    \n",
    "    #for each filter in filters, we create a branch\n",
    "    if len(f) > 1:\n",
    "        num_filter = len(f)\n",
    "        convolutions = [None] * num_filter\n",
    "        pools = [None] * num_filter\n",
    "        flattens = [None] * num_filter\n",
    "        for index in range(len(f)):\n",
    "            convolutions[index] = Conv1D(2, kernel_size=(f[index],),\\\n",
    "                                        activation='relu',\\\n",
    "                                        input_shape=(len(x_train),))(x)\n",
    "            pools[index] = MaxPooling1D(pool_size=4)(convolutions[index])\n",
    "            flattens[index] = Flatten()(pools[index])\n",
    "\n",
    "        merge = concatenate(flattens)\n",
    "        \n",
    "    else: #only one filter\n",
    "        conv1 = Conv1D(2, kernel_size = (f[0],), activation='relu'\\\n",
    "                         ,input_shape=(len(x_train),))(x)\n",
    "        pool1 = MaxPooling1D(pool_size=4)(conv1)\n",
    "        merge = Flatten()(pool1)\n",
    "    \n",
    "    hidden1 = Dense(10, activation='relu')(merge)\n",
    "    output = Dense(1, activation='sigmoid')(hidden1)\n",
    "    model = Model(inputs = visible, outputs = output)\n",
    "    print model.summary()\n",
    "    #save model into dict\n",
    "    model_dictionary[model] = f\n",
    "    break\n",
    "    #plot_model(model, to_file=str(f)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using model with filter(s): [5, 6]\n",
      "51160/51160 [==============================] - 3s 67us/step\n",
      "training accuracy: 88.332682\n",
      "119376/119376 [==============================] - 8s 69us/step\n",
      "testing accuracy: 86.694143\n",
      "\n",
      "\n",
      "using model with filter(s): [6, 6, 6]\n",
      "51160/51160 [==============================] - 3s 60us/step\n",
      "training accuracy: 88.332682\n",
      "119376/119376 [==============================] - 7s 61us/step\n",
      "testing accuracy: 86.694143\n",
      "\n",
      "\n",
      "using model with filter(s): [6, 7]\n",
      "51160/51160 [==============================] - 3s 62us/step\n",
      "training accuracy: 88.332682\n",
      "119376/119376 [==============================] - 7s 62us/step\n",
      "testing accuracy: 86.694143\n",
      "\n",
      "\n",
      "using model with filter(s): [5, 6, 7]\n",
      "51160/51160 [==============================] - 4s 70us/step\n",
      "training accuracy: 91.872557\n",
      "119376/119376 [==============================] - 7s 62us/step\n",
      "testing accuracy: 88.048687\n",
      "\n",
      "\n",
      "using model with filter(s): [6]\n",
      "51160/51160 [==============================] - 3s 62us/step\n",
      "training accuracy: 91.872557\n",
      "119376/119376 [==============================] - 8s 63us/step\n",
      "testing accuracy: 88.048687\n",
      "\n",
      "\n",
      "using model with filter(s): [6, 6]\n",
      "51160/51160 [==============================] - 3s 63us/step\n",
      "training accuracy: 91.872557\n",
      "119376/119376 [==============================] - 8s 64us/step\n",
      "testing accuracy: 88.048687\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compile, fit and test the models\n",
    "for k, v in model_dictionary.iteritems():\n",
    "    print 'using model with filter(s):' , v\n",
    "    print 'before compile', k\n",
    "    k.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    k.fit(x_train,y_train, verbose = 0)\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('training accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    #evaluate on test\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('testing accuracy: %f' % (accuracy*100))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different accuracies achieved by the above filters are below:\n",
    "\n",
    "| filters used | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      6     |         91.87       |      88.05        |\n",
    "|      6,6      |         91.87       |       88.05      |\n",
    "|      6,6,6      |        88.33        |       86.69      |\n",
    "|      5,6      |       88.33         |       86.69     |\n",
    "|      6,7      |      88.33          |       86.69      |\n",
    "|      5,6,7    |       91.87         |       88.05      |\n",
    "\n",
    "Due to the low number of samples used in training/testing, the neural network may have achieved its maximum fit with the above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis: Feature maps\n",
    "\n",
    "WIP:\n",
    "According to the article, feature maps should range between 100-600. We will spot check some of them and check the accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features is:  10\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 135s 3ms/step - loss: 0.3880 - acc: 0.8298\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 127s 2ms/step - loss: 0.2525 - acc: 0.8977\n",
      "51160/51160 [==============================] - 4s 71us/step\n",
      "Training Accuracy: 92.724785\n",
      "119376/119376 [==============================] - 7s 60us/step\n",
      "Testing Accuracy: 88.150885\n",
      "\n",
      "\n",
      "num_features is:  50\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 131s 3ms/step - loss: 0.3473 - acc: 0.8519\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 128s 3ms/step - loss: 0.2263 - acc: 0.9118\n",
      "51160/51160 [==============================] - 4s 76us/step\n",
      "Training Accuracy: 94.100860\n",
      "119376/119376 [==============================] - 8s 66us/step\n",
      "Testing Accuracy: 89.037998\n",
      "\n",
      "\n",
      "num_features is:  100\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 133s 3ms/step - loss: 0.3441 - acc: 0.8541\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 131s 3ms/step - loss: 0.2223 - acc: 0.9133\n",
      "51160/51160 [==============================] - 5s 90us/step\n",
      "Training Accuracy: 94.210321\n",
      "119376/119376 [==============================] - 10s 81us/step\n",
      "Testing Accuracy: 88.908993\n",
      "\n",
      "\n",
      "num_features is:  200\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 142s 3ms/step - loss: 0.3376 - acc: 0.8590\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 2700s 53ms/step - loss: 0.2225 - acc: 0.9126\n",
      "51160/51160 [==============================] - 8s 150us/step\n",
      "Training Accuracy: 94.337373\n",
      "119376/119376 [==============================] - 16s 137us/step\n",
      "Testing Accuracy: 89.234855\n",
      "\n",
      "\n",
      "num_features is:  400\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 179s 4ms/step - loss: 0.3304 - acc: 0.8602\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 175s 3ms/step - loss: 0.2179 - acc: 0.9158\n",
      "51160/51160 [==============================] - 15s 296us/step\n",
      "Training Accuracy: 94.757623\n",
      "119376/119376 [==============================] - 34s 282us/step\n",
      "Testing Accuracy: 89.419146\n",
      "\n",
      "\n",
      "num_features is:  600\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 180s 4ms/step - loss: 0.3332 - acc: 0.8607\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 176s 3ms/step - loss: 0.2181 - acc: 0.9145\n",
      "51160/51160 [==============================] - 22s 436us/step\n",
      "Training Accuracy: 94.689210\n",
      "119376/119376 [==============================] - 51s 425us/step\n",
      "Testing Accuracy: 89.398204\n",
      "\n",
      "\n",
      "num_features is:  800\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 209s 4ms/step - loss: 0.3245 - acc: 0.8633\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 190s 4ms/step - loss: 0.2090 - acc: 0.9194\n",
      "51160/51160 [==============================] - 28s 552us/step\n",
      "Training Accuracy: 95.048866\n",
      "119376/119376 [==============================] - 64s 537us/step\n",
      "Testing Accuracy: 89.558203\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's use the above neural network to find the optimal window size\n",
    "feature_maps = [10,50,100,200,400,600,800]\n",
    "models = []\n",
    "\n",
    "for feature in feature_maps:\n",
    "    print 'num_features is: ', feature\n",
    "    #create the CNN\n",
    "    vect_dimension = 4 \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "    model.add(Conv1D(feature, kernel_size=6, padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model jsing 50% of the data\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    #evaluate on test\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Testing Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    model.save('model_feature_map_'+str(k))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features is:  700\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 185s 4ms/step - loss: 0.3301 - acc: 0.8623\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 179s 4ms/step - loss: 0.2219 - acc: 0.9126\n",
      "51160/51160 [==============================] - 26s 500us/step\n",
      "Training Accuracy: 94.419468\n",
      "119376/119376 [==============================] - 56s 468us/step\n",
      "Testing Accuracy: 88.819361\n",
      "\n",
      "\n",
      "num_features is:  900\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 200s 4ms/step - loss: 0.3327 - acc: 0.8602\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 198s 4ms/step - loss: 0.2177 - acc: 0.9157\n",
      "51160/51160 [==============================] - 30s 577us/step\n",
      "Training Accuracy: 94.569977\n",
      "119376/119376 [==============================] - 67s 560us/step\n",
      "Testing Accuracy: 89.378099\n",
      "\n",
      "\n",
      "num_features is:  1000\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 217s 4ms/step - loss: 0.3322 - acc: 0.8613\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 216s 4ms/step - loss: 0.2215 - acc: 0.9145\n",
      "51160/51160 [==============================] - 33s 636us/step\n",
      "Training Accuracy: 92.931978\n",
      "119376/119376 [==============================] - 76s 640us/step\n",
      "Testing Accuracy: 87.621465\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's use the above neural network to find the optimal window size\n",
    "feature_maps = [700,900,1000]\n",
    "models = []\n",
    "\n",
    "for feature in feature_maps:\n",
    "    print 'num_features is: ', feature\n",
    "    #create the CNN\n",
    "    vect_dimension = 4 \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "    model.add(Conv1D(feature, kernel_size=6, padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model jsing 50% of the data\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    #evaluate on test\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Testing Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    model.save('model_feature_map_'+str(k))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP:<br>\n",
    "we see from the below table that we achieve the highest testing accuracy with 800 feature maps per convolution.\n",
    "\n",
    "| number of features | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      10    |         92.72      |      88.15        |\n",
    "|      50     |         94.10     |       89.03      |\n",
    "|      100      |        94.21        |       88.91      |\n",
    "|      200      |       94.33         |       89.23    |\n",
    "|      400      |      94.75         |       89.41|\n",
    "|      600    |       94.68         |       89.39      |\n",
    "|      700    |       94.41         |       88.82      |\n",
    "|      800    |       95.05         |       89.55      |\n",
    "|      900    |       94.57         |       89.37      |\n",
    "|      1000    |       92.93         |      87.62      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activation functions\n",
    "\n",
    "According to the article, RELU and TANH is the best suited functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation function is:  relu\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 201s 4ms/step - loss: 0.3350 - acc: 0.8589\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 208s 4ms/step - loss: 0.2192 - acc: 0.9145\n",
      "51160/51160 [==============================] - 28s 554us/step\n",
      "Training Accuracy: 93.960125\n",
      "119376/119376 [==============================] - 64s 539us/step\n",
      "Testing Accuracy: 88.866271\n",
      "\n",
      "\n",
      "activation function is:  tanh\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 206s 4ms/step - loss: 0.3438 - acc: 0.8539\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 204s 4ms/step - loss: 0.2372 - acc: 0.9065\n",
      "51160/51160 [==============================] - 34s 669us/step\n",
      "Training Accuracy: 94.317826\n",
      "119376/119376 [==============================] - 71s 598us/step\n",
      "Testing Accuracy: 88.165125\n",
      "\n",
      "\n",
      "activation function is:  sigmoid\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 207s 4ms/step - loss: 3.5416 - acc: 0.7778\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 210s 4ms/step - loss: 3.5422 - acc: 0.7778\n",
      "51160/51160 [==============================] - 33s 642us/step\n",
      "Training Accuracy: 77.781470\n",
      "119376/119376 [==============================] - 75s 627us/step\n",
      "Testing Accuracy: 77.623643\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's use the above neural network to find the optimal window size\n",
    "activation_functions = ['relu','tanh', 'sigmoid']\n",
    "models = []\n",
    "\n",
    "for func in activation_functions:\n",
    "    print 'activation function is: ', func\n",
    "    #create the CNN\n",
    "    vect_dimension = 4 \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "    model.add(Conv1D(800, kernel_size=6, padding='same',activation=func))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model jsing 50% of the data\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    #evaluate on test\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Testing Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    model.save('model_feature_map_'+str(k))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| activation function | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      relu    |         93.96      |      88.87        |\n",
    "|      tanh     |         94.50     |       87.66      |\n",
    "|      sigmoid     |         94.50     |       87.66      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 81s 2ms/step - loss: 0.4366 - acc: 0.7876\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 84s 2ms/step - loss: 0.3402 - acc: 0.8446\n",
      "34107/34107 [==============================] - 1s 38us/step\n",
      "Accuracy: 87.375026\n"
     ]
    }
   ],
   "source": [
    "# fit the model jsing 50% of the data\n",
    "model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "#evaluate on test\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 1927s 11ms/step - loss: 0.3021 - acc: 0.8730\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 486s 3ms/step - loss: 0.2207 - acc: 0.9134\n",
      "170536/170536 [==============================] - 9s 54us/step\n",
      "Accuracy: 93.032556\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397918/397918 [==============================] - 22s 56us/step\n",
      "Accuracy: 90.390482\n"
     ]
    }
   ],
   "source": [
    "#put on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 4)            7489984   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               42000     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,532,085\n",
      "Trainable params: 7,532,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create RNN\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "rnn_model.add(LSTM(100))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(rnn_model.summary())\n",
    "#model.fit(x_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 1241s 7ms/step - loss: 0.5254 - acc: 0.7805\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 1205s 7ms/step - loss: 0.5066 - acc: 0.7815\n",
      "170536/170536 [==============================] - 8s 50us/step\n",
      "Accuracy: 77.900267\n"
     ]
    }
   ],
   "source": [
    "rnn_model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282400/397918 [====================>.........] - ETA: 1:49"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6a62db98ec05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#put on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m    920\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1690\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m                                steps=steps)\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#put on test set\n",
    "loss, accuracy = rnn_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Here, we will try different parameters to tune the CNN and check the performance. \n",
    "The first cell will focus on how many dimensions to use and the second cell will focus on different optimizers/loss functions and the result will be plotted in a heatmap to visualize the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dim:  softmax\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 448s 3ms/step - loss: 3.5128 - acc: 0.7797\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 471s 3ms/step - loss: 3.5128 - acc: 0.7797\n",
      "170536/170536 [==============================] - 4s 26us/step\n",
      "Training Accuracy: 77.965356\n",
      "397918/397918 [==============================] - 11s 26us/step\n",
      "Test Accuracy: 78.111068\n",
      "\n",
      "\n",
      "dim:  relu\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 450s 3ms/step - loss: 0.4290 - acc: 0.8162\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 448s 3ms/step - loss: 0.3487 - acc: 0.8478\n",
      "170536/170536 [==============================] - 5s 28us/step\n",
      "Training Accuracy: 78.470235\n",
      "397918/397918 [==============================] - 11s 28us/step\n",
      "Test Accuracy: 75.204188\n",
      "\n",
      "\n",
      "dim:  tanh\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 439s 3ms/step - loss: 0.4380 - acc: 0.8170\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 426s 3ms/step - loss: 0.3396 - acc: 0.8584\n",
      "170536/170536 [==============================] - 5s 27us/step\n",
      "Training Accuracy: 87.092461\n",
      "397918/397918 [==============================] - 11s 27us/step\n",
      "Test Accuracy: 85.298981\n",
      "\n",
      "\n",
      "dim:  sigmoid\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 449s 3ms/step - loss: 0.3112 - acc: 0.8682\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 432s 3ms/step - loss: 0.2351 - acc: 0.9068\n",
      "170536/170536 [==============================] - 5s 27us/step\n",
      "Training Accuracy: 92.204579\n",
      "397918/397918 [==============================] - 11s 27us/step\n",
      "Test Accuracy: 89.890882\n",
      "\n",
      "\n",
      "dim:  linear\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 430s 3ms/step - loss: 0.4437 - acc: 0.8170\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 436s 3ms/step - loss: 0.3315 - acc: 0.8427\n",
      "170536/170536 [==============================] - 5s 27us/step\n",
      "Training Accuracy: 82.598396\n",
      "397918/397918 [==============================] - 11s 27us/step\n",
      "Test Accuracy: 80.199438\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#different activation functions\n",
    "#create multiple CNNs with different hyperparameters\n",
    "\n",
    "activation_functions = ['softmax', 'relu', 'tanh', 'sigmoid', 'linear']\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "for af in activation_functions:\n",
    "    print 'dim: ', af\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 4, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=af))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "    #put on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    models_accuracy[model]['training_error'] = train_accuracy\n",
    "    models_accuracy[model]['test_error']=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 4, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create multiple CNNs with different hyperparameters\n",
    "\n",
    "vect_dimensions = [2,4,6,8]\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "for dim in vect_dimensions:\n",
    "    print 'dim: ', dim\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, dim, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "    #put on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    models_accuracy[model]['training_error'] = train_accuracy\n",
    "    models_accuracy[model]['test_error']=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the mdoels which was just created\n",
    "post_name = 1\n",
    "for k,v in models_accuracy.iteritems():\n",
    "    out_name = 'keras_model_'+str(post_name)\n",
    "    k.save(out_name)\n",
    "    post_name+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizers = ['adam', 'sgd', 'Adagrad', 'Adadelta']\n",
    "losses = ['mean_squared_error', 'cosine_proximity', 'binary_crossentropy']\n",
    "metrics=['acc']\n",
    "\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for loss in losses:\n",
    "        print 'optimizer: ', optimizer\n",
    "        print 'loss: ', loss, '\\n'\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, 2, input_length=max_length))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        #model compilation. add loss function\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "        # fit the model\n",
    "        model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "        # evaluate the model\n",
    "        train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "        print('\\n Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "        #put on test set\n",
    "        test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "        print('\\n Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        #models_accuracy[model]['training_error'] = train_accuracy\n",
    "        #models_accuracy[model]['test_error']=test_accuracy\n",
    "        models_accuracy[optimizer][loss] = defaultdict(dict)\n",
    "        models_accuracy[optimizer][loss]['train_acc']=train_accuracy\n",
    "        models_accuracy[optimizer][loss]['test_acc']=test_accuracy\n",
    "\n",
    "        \n",
    "        print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('hyper_parameter_tuning.pickle', 'wb') as handle:\n",
    "    pickle.dump(models_accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('hyper_parameter_tuning.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print models_accuracy == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
