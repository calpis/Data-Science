{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Keras on Amazon's fine food review Part 1\n",
    "\n",
    "   This notebook will use the data which was prepared in the data_prepation.ipynb and test the accuracy using a couple baseline models.  Since the goal of the prediction is basically a classification problem (whether or not a review is positive or not), we will utilize logistic regression and support vector machines to do this.  <br>\n",
    "\n",
    "   The notebook will be organized as below:\n",
    "- Create a bag of words representation to encode the text into numerical vectors.\n",
    "- Use the numerical vectors and fit them onto a logistic regressor and SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#data importing/wrangling\n",
    "import pandas as pd\n",
    "\n",
    "#text processing \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#CNN modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, Conv1D, MaxPooling1D, MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "#data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "#only required first time\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "input_location = '/Users/momori/data/reviews_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(input_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'Id', u'ProductId', u'UserId', u'ProfileName',\n",
       "       u'HelpfulnessNumerator', u'HelpfulnessDenominator', u'Score', u'Time',\n",
       "       u'Summary', u'Text', u'HelpfulnessRatio', u'avg_score',\n",
       "       u'normalized_score', u'positive_review'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create documents and labels to train the model later\n",
    "docs = data['Text']\n",
    "labels = data['positive_review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create baseline model <br>\n",
    "\n",
    "To analyze how effective neural network models are for this dataset, we will be creating a logistic regression model and a SVM model as baselines to compare with the neural networks.  For these, the dataset will be encoded using the popular and straightforward bag of words method after removing stopwords using the list provided by nltk library.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag of words representation if one way to encode language into numerical vectors.  This happens by first creating a list of all vocabularies used by the entire dataset.  Then for each data point, the above list is initialized with all zeros, then the corresponding index's entry is incremented for each word in the data point.  <br>\n",
    "\n",
    "For example, consider the sentence 'I eat an apple' with a vocabulary list of 'I', 'eat', 'an', 'apple', 'orange'.  The cardinality of the vocabulary list is five, so there will be a 1x5 vector representation of each datapoint.  In the above example's case, the representation will be [1,1,1,1,0].  Similarly, for the sentence 'I eat an orange,' the representaiton would be [1,1,1,0,1].<br>\n",
    "\n",
    "There are a few issues with this representation which requires some preprocessing of the data.  First off, the existence of stopwords heavily bias the resulting vectors.  For example, most sentences will have very common words such as 'the', 'a', 'an', punctuations and the likes.  These words are removed from the original data source so the models can only look at significant terms, and the stopwords are provided by the NLTK library.  Another point to note is that the vectorizer class used to create the bag of words representation will differentiate between terms of different cases, such as 'apple' and 'Apple.' Hence, before we start the vectorization process, the datasource will be turned into all lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions\n",
    "def bag_of_words(series_text):\n",
    "    #return series_text.apply(remove_stopwords)\n",
    "    pass\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = keep_letters(text)\n",
    "    text = to_lower(text)\n",
    "    words = [w for w in text if not w in cached_stop_words]\n",
    "    return(\" \".join(words))\n",
    "\n",
    "def to_lower(text):\n",
    "    return [w.lower() for w in text.split()]\n",
    "\n",
    "def keep_letters(text):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    return letters_only\n",
    "\n",
    "def calculate_accuracy(prediction, actual):\n",
    "    zipped = zip(prediction, actual)\n",
    "    acc = [i for i in zipped if i[0]==i[1]]\n",
    "    return len(acc)/float(len(zipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bought several vitality canned dog food produc...\n",
       "1    product arrived labeled jumbo salted peanuts p...\n",
       "2    confection around centuries light pillowy citr...\n",
       "3    looking secret ingredient robitussin believe f...\n",
       "4    great taffy great price wide assortment yummy ...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the text to lower case and remove stopwords\n",
    "cached_stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "reduced_text = docs.apply(remove_stopwords)\n",
    "\n",
    "reduced_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a processed dataset without punctuations, all lowerccase for easy comparison and without stopwords.  Now we will create a bag of words representation with this text using the CountVectorizer.  <br>\n",
    "\n",
    "The below CountVectorizer is part sklearn.feature_extraction library and provides a simple way to create a sparse matrix of token counts from the original text. As seen below, the vectorizer created a matrix with the dimension (568454, 110979), where there are equivalent number of rows as the data points in the source, and recognized 110979 different vocabularies in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(reduced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 110979)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the baseline models\n",
    "\n",
    "\n",
    "Now that we have the bag of words representation for the text summaries, the goal of the below cells will be to fit a logistic regression and SVM model to measure the accuracy, to set the baseline.  The processed data will be split into a train/test split, with 70% of the data as the training set.  Then the models will be fit with the training data, and accuracy will be measured on the testing set.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data\n",
    "train_size = 0.7\n",
    "\n",
    "x_train_base, x_test_base, y_train_base, y_test_base = train_test_split(\n",
    "    train_data_features, labels, test_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'penalty': ['l1', 'l2'], 'max_iter': [10, 100, 200]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_parameters = [{'penalty':['l1','l2'], 'max_iter':[10,100, 200]}]\n",
    "\n",
    "grid = GridSearchCV(linear_model.LogisticRegression(), lr_parameters)\n",
    "grid.fit(x_train_base, y_train_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.889724163813 {'penalty': 'l1', 'max_iter': 10}\n",
      "0.887859454895 {'penalty': 'l2', 'max_iter': 10}\n",
      "0.8895775672 {'penalty': 'l1', 'max_iter': 100}\n",
      "0.89036918891 {'penalty': 'l2', 'max_iter': 100}\n",
      "0.889595158793 {'penalty': 'l1', 'max_iter': 200}\n",
      "0.89036918891 {'penalty': 'l2', 'max_iter': 200}\n"
     ]
    }
   ],
   "source": [
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "\n",
    "for mean, params in zip(means,grid.cv_results_['params']):\n",
    "    print mean, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_str(row):\n",
    "    return row['params'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'penalty': 'l1', 'max_iter': 10}</td>\n",
       "      <td>0.889724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'penalty': 'l2', 'max_iter': 10}</td>\n",
       "      <td>0.887859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'penalty': 'l1', 'max_iter': 100}</td>\n",
       "      <td>0.889578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'penalty': 'l2', 'max_iter': 100}</td>\n",
       "      <td>0.890369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'penalty': 'l1', 'max_iter': 200}</td>\n",
       "      <td>0.889595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'penalty': 'l2', 'max_iter': 200}</td>\n",
       "      <td>0.890369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               params  accuracy\n",
       "0   {'penalty': 'l1', 'max_iter': 10}  0.889724\n",
       "1   {'penalty': 'l2', 'max_iter': 10}  0.887859\n",
       "2  {'penalty': 'l1', 'max_iter': 100}  0.889578\n",
       "3  {'penalty': 'l2', 'max_iter': 100}  0.890369\n",
       "4  {'penalty': 'l1', 'max_iter': 200}  0.889595\n",
       "5  {'penalty': 'l2', 'max_iter': 200}  0.890369"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_df = pd.DataFrame(zip(grid.cv_results_['params'], means), columns=['params','accuracy'])\n",
    "lr_df['params']=lr_df['params'].astype(str)\n",
    "lr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure SVM Model\n",
    "\n",
    "The complexity of training a SVM model is quadratic, and could get as bad as O(n^3).  Hence it is recommended that we split the data into manageable chunks and create an ensemble of SVMs (https://stackoverflow.com/questions/31681373/making-svm-run-faster-in-python).  We will utilize the BaggingClassifier wrapper to achieve this and use the SGDClassifier as that was the only implementation of SVM which would finish in a reasonable time. The other implementation would take roughly 4~6 hours to finish on my local laptop(SVC and LinearSVC). The other thing to note is that sklearn does not support GPU computing, so the SVM computation time could not be improved by running it on an AWS instance with GPU.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'penalty': ['l1', 'l2'], 'loss': ['hinge', 'log', 'squared_hinge']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_parameters = [{'penalty':['l1','l2'], 'loss':['hinge', 'log', 'squared_hinge']}]\n",
    "\n",
    "svm_grid = GridSearchCV(SGDClassifier(), svm_parameters)\n",
    "svm_grid.fit(x_train_base, y_train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'penalty': 'l1', 'loss': 'hinge'}</td>\n",
       "      <td>0.889724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'penalty': 'l2', 'loss': 'hinge'}</td>\n",
       "      <td>0.887859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'penalty': 'l1', 'loss': 'log'}</td>\n",
       "      <td>0.889578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'penalty': 'l2', 'loss': 'log'}</td>\n",
       "      <td>0.890369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'penalty': 'l1', 'loss': 'squared_hinge'}</td>\n",
       "      <td>0.889595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'penalty': 'l2', 'loss': 'squared_hinge'}</td>\n",
       "      <td>0.890369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       params  accuracy\n",
       "0  {'penalty': 'l1', 'loss': 'hinge'}          0.889724\n",
       "1  {'penalty': 'l2', 'loss': 'hinge'}          0.887859\n",
       "2  {'penalty': 'l1', 'loss': 'log'}            0.889578\n",
       "3  {'penalty': 'l2', 'loss': 'log'}            0.890369\n",
       "4  {'penalty': 'l1', 'loss': 'squared_hinge'}  0.889595\n",
       "5  {'penalty': 'l2', 'loss': 'squared_hinge'}  0.890369"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "svm_df = pd.DataFrame(zip(svm_grid.cv_results_['params'],means),columns=['params','accuracy'])\n",
    "svm_df.params = svm_df.params.astype(str)\n",
    "svm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above section, we created a baseline model using the following steps <br>\n",
    "1) Use the bag of words algorithm to encode the texts into numerical vectors <br>\n",
    "2) Use logistic regression and support vector machines to create a baseline model.<br>\n",
    "<br>\n",
    "\n",
    "Interestingly enough, logistic regression and SVM both achieve a very respectable accuracy of ~89%. With further parameter tuning, the accuracy may improve slightly.  The interesting analysis to take note here is that logistic regressions fit the dataset relatively well, which can denote that the documents and the labels fit a pretty linear relationship.  This may mean that majority of the text can be taken for what it is without sarcasm.  (If there were a lot of sarcasm and dependency on the tone of the review, then the relationship may not have been linear).  <br>\n",
    "\n",
    "We will now approach the same dataset using a different encoding algorithm and combining that with neural networks in the next notebook to measure the increase in accuracy in NeuralNets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Below for testing purposes. To be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embedding\n",
    "\n",
    "Here we will use the word2vec embedding algorithm implementation by keras.  Since neural networks can only take in numerical values and vectors to train from, the texts will need to be encoded into numerical values.  After each review is encoded, the lengths of the review will be analyzed because the shorter review vedctors needs to be 0-padded to match the longest review vector.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 17, 125, 319, 7, 1, 4940, 523, 103, 52, 204, 3, 17, 117, 28, 41, 5, 30, 7, 29, 183, 1, 38, 629, 48, 26, 4, 2636, 58, 4, 1183, 448, 3, 6, 619, 99, 13, 5266, 8, 1777, 3, 94, 8695, 9, 38, 99, 58, 140]\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec embedding\n",
    "\n",
    "#use the tokenizer API provided by Keras to turn documents into sequences\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "#encode the documents into integers\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a list of lengths of each of the encoded_docs to find out the longest length, \n",
    "#and see if any 0 padding is required\n",
    "doc_lengths = [len(doc) for doc in encoded_docs]\n",
    "max_doc_length = max(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  percentile of document length: 57.0\n",
      "60  percentile of document length: 70.0\n",
      "70  percentile of document length: 88.0\n",
      "80  percentile of document length: 114.0\n",
      "90  percentile of document length: 164.0\n",
      "99  percentile of document length: 398.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMIIgA4ARBBkAjPgf6qs6AGZ2iOYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fbcb790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's put the lengths into a boxplot to see the distributions of the lengths\n",
    "np_array = np.array(doc_lengths)\n",
    "\n",
    "for i in [50,60,70,80,90,99]:\n",
    "    print i, \" percentile of document length:\", np.percentile(np_array,i)\n",
    "    \n",
    "ax = sns.boxplot(x=doc_lengths, showfliers=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see from the above plot that the third quantile of document lengths is around 200 words or so.  We can test using this as the max document length and ignoring the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE4WDiIiYKBxERMRE4SAiIiYKBxERMfl/Bn+HBYA2KHAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a28450310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's put the lengths into a histogram to see the distributions of the lengths\n",
    "plt.hist([x for x in doc_lengths if x <= 200], bins=30)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cap the maximum length of the reviews to 200 words, as that will give us a good sense of the overall data without having to zero pad the short reviews to match really long reviews.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#zero pad the shorter texts\n",
    "max_length = 200\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872496\n"
     ]
    }
   ],
   "source": [
    "#find the vocab size from tokenizer\n",
    "vocab_size = t.word_counts[max(t.word_counts,key=t.word_counts.get)]\n",
    "print vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "now that we have the data, we can split this into train/test and create a CNN using Keras. <br>\n",
    "The CNN will be layered as follows:<br>\n",
    "\n",
    "- The embedding layer, which is the input layer consisting of the embedded word vectors.\n",
    "- A Flatten layer, to make the 2-dimensional input into a 1-dimensional output.\n",
    "- A Dense layer, which will create a fully connected layer with 1 node for the output.\n",
    "- activation function is sigmoid, because this will give us a binary output, either positive review or non-positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data\n",
    "train_size = 0.7\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    padded_docs, labels, test_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 8)            14979968  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1601      \n",
      "=================================================================\n",
      "Total params: 14,981,569\n",
      "Trainable params: 14,981,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# #create the CNN\n",
    "# vect_dimension = 8\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# #model compilation. add loss function\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# # check model\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 200, 32)           59919872  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 200, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 3201      \n",
      "=================================================================\n",
      "Total params: 59,926,177\n",
      "Trainable params: 59,926,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create the CNN\n",
    "vect_dimension = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "model.add(Conv1D(32, kernel_size=3, padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#model compilation. add loss function\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# check model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 4412s - loss: 0.3039 - acc: 0.8735  \n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 4079s - loss: 0.1896 - acc: 0.9280  \n",
      "169984/170536 [============================>.] - ETA: 0sAccuracy: 96.052446\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397312/397918 [============================>.] - ETA: 0sAccuracy: 91.465076\n"
     ]
    }
   ],
   "source": [
    "#put on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc after epoch 1 \n",
    "\n",
    "vect dim 8, filter_size 8: 8623\n",
    "8, 32: 88, 91, 94, 91\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Here, we will try different parameters to tune the CNN and check the performance. \n",
    "The first cell will focus on how many dimensions to use and the second cell will focus on different optimizers/loss functions and the result will be plotted in a heatmap to visualize the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim:  2\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 280s - loss: 0.3267 - acc: 0.8595   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 280s - loss: 0.2442 - acc: 0.9024   \n",
      "168416/170535 [============================>.] - ETA: 0sTraining Accuracy: 91.319670\n",
      "396928/397917 [============================>.] - ETA: 0sTest Accuracy: 89.581244\n",
      "\n",
      "\n",
      "dim:  4\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 547s - loss: 0.3149 - acc: 0.8657   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 545s - loss: 0.2360 - acc: 0.9057   \n",
      "169280/170535 [============================>.] - ETA: 0sTraining Accuracy: 91.904888\n",
      "396128/397917 [============================>.] - ETA: 0sTest Accuracy: 89.849642\n",
      "\n",
      "\n",
      "dim:  6\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 799s - loss: 0.3077 - acc: 0.8701   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 801s - loss: 0.2301 - acc: 0.9088   \n",
      "170336/170535 [============================>.] - ETA: 0sTraining Accuracy: 92.755153\n",
      "395520/397917 [============================>.] - ETA: 0sTest Accuracy: 90.160511\n",
      "\n",
      "\n",
      "dim:  8\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 1063s - loss: 0.3040 - acc: 0.8717  \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 1087s - loss: 0.2265 - acc: 0.9102  - ETA: 9s - loss: - ETA: 5s - \n",
      "169344/170535 [============================>.] - ETA: 0sTraining Accuracy: 92.724074\n",
      "397312/397917 [============================>.] - ETA: 0sTest Accuracy: 90.017265\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create multiple CNNs with different hyperparameters\n",
    "\n",
    "vect_dimensions = [2,4,6,8]\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "for dim in vect_dimensions:\n",
    "    print 'dim: ', dim\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, dim, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "    #put on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    models_accuracy[model]['training_error'] = train_accuracy\n",
    "    models_accuracy[model]['test_error']=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d65a38009c98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 4, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create multiple CNNs with different activations\n",
    "\n",
    "vect_dimensions = [2,4,6,8]\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "for dim in vect_dimensions:\n",
    "    print 'dim: ', dim\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 4, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "    #put on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    models_accuracy[model]['training_error'] = train_accuracy\n",
    "    models_accuracy[model]['test_error']=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the mdoels which was just created\n",
    "post_name = 1\n",
    "for k,v in models_accuracy.iteritems():\n",
    "    out_name = 'keras_model_'+str(post_name)\n",
    "    k.save(out_name)\n",
    "    post_name+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling.ipynb         keras_model_1          keras_model_4\r\n",
      "Proposal.md            keras_model_2          my_model.h5\r\n",
      "data_preparation.ipynb keras_model_3\r\n"
     ]
    }
   ],
   "source": [
    "#chcek for the model files\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer:  adam\n",
      "loss:  mean_squared_error \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 243s - loss: 0.1019 - acc: 0.8621   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 236s - loss: 0.0722 - acc: 0.9052   \n",
      "167328/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 92.103087\n",
      "397120/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 90.006207\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  adam\n",
      "loss:  cosine_proximity \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 231s - loss: -0.7797 - acc: 0.7791   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 231s - loss: -0.7797 - acc: 0.7797   \n",
      "169632/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 77.970505\n",
      "396224/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 78.108751\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  adam\n",
      "loss:  binary_crossentropy \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 231s - loss: 0.3266 - acc: 0.8599   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 233s - loss: 0.2442 - acc: 0.9016   \n",
      "170016/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 91.474477\n",
      "397792/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 89.680009\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  sgd\n",
      "loss:  mean_squared_error \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 122s - loss: 0.1733 - acc: 0.7781   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 122s - loss: 0.1706 - acc: 0.7797   \n",
      "169920/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 77.970505\n",
      "395680/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 78.108751\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  sgd\n",
      "loss:  cosine_proximity \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 124s - loss: -0.7797 - acc: 0.7108   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 123s - loss: -0.7797 - acc: 0.7108   \n",
      "169696/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 71.080423\n",
      "394880/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 70.969071\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  sgd\n",
      "loss:  binary_crossentropy \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 128s - loss: 0.5254 - acc: 0.7791   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 123s - loss: 0.5170 - acc: 0.7797   \n",
      "170496/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 77.970505\n",
      "396960/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 78.108751\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adagrad\n",
      "loss:  mean_squared_error \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 181s - loss: 0.1206 - acc: 0.8302   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 182s - loss: 0.0934 - acc: 0.8743   \n",
      "168544/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 88.134987\n",
      "396704/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 87.423005\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adagrad\n",
      "loss:  cosine_proximity \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 187s - loss: -0.7797 - acc: 0.7796   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 182s - loss: -0.7797 - acc: 0.7797   \n",
      "170368/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 77.970505\n",
      "397824/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 78.108751\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adagrad\n",
      "loss:  binary_crossentropy \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 181s - loss: 0.3811 - acc: 0.8254   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 186s - loss: 0.3055 - acc: 0.8689   - ETA: 1s - loss:\n",
      "169760/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 87.827719\n",
      "396352/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 87.222461\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adadelta\n",
      "loss:  mean_squared_error \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 311s - loss: 0.1478 - acc: 0.7943   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 314s - loss: 0.1089 - acc: 0.8479   \n",
      "169600/170535 [============================>.] - ETA: 0s ETA\n",
      " Training Accuracy: 86.083795\n",
      "395072/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 85.772661\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adadelta\n",
      "loss:  cosine_proximity \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 309s - loss: -0.7797 - acc: 0.5201   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 307s - loss: -0.7797 - acc: 0.5224   \n",
      "167936/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 52.356408\n",
      "395616/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 52.221192\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "optimizer:  Adadelta\n",
      "loss:  binary_crossentropy \n",
      "\n",
      "Epoch 1/2\n",
      "170535/170535 [==============================] - 302s - loss: 0.4450 - acc: 0.7977   \n",
      "Epoch 2/2\n",
      "170535/170535 [==============================] - 302s - loss: 0.3384 - acc: 0.8505   \n",
      "168768/170535 [============================>.] - ETA: 0s\n",
      " Training Accuracy: 86.259712\n",
      "397600/397917 [============================>.] - ETA: 0s\n",
      " Test Accuracy: 85.881226\n",
      "\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optimizers = ['adam', 'sgd', 'Adagrad', 'Adadelta']\n",
    "losses = ['mean_squared_error', 'cosine_proximity', 'binary_crossentropy']\n",
    "metrics=['acc']\n",
    "\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for loss in losses:\n",
    "        print 'optimizer: ', optimizer\n",
    "        print 'loss: ', loss, '\\n'\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, 2, input_length=max_length))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        #model compilation. add loss function\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "        # fit the model\n",
    "        model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "        # evaluate the model\n",
    "        train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "        print('\\n Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "        #put on test set\n",
    "        test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "        print('\\n Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        #models_accuracy[model]['training_error'] = train_accuracy\n",
    "        #models_accuracy[model]['test_error']=test_accuracy\n",
    "        models_accuracy[optimizer][loss] = defaultdict(dict)\n",
    "        models_accuracy[optimizer][loss]['train_acc']=train_accuracy\n",
    "        models_accuracy[optimizer][loss]['test_acc']=test_accuracy\n",
    "\n",
    "        \n",
    "        print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with open('hyper_parameter_tuning.pickle', 'wb') as handle:\n",
    "    pickle.dump(models_accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('hyper_parameter_tuning.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print models_accuracy == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
