{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for neural network training. Will be run on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Keras on Amazon's fine food review\n",
    "\n",
    "This notebook will create a convolutional neural network (CNN) off of the fine food review to suggest if the review is a positive (4 or 5 star rating) or a non-positive (1, 2, or 3 star rating).  What we aim here is to use the word2vec embedding implementation in Keras to convert the text into word vectors, and train a CNN with it which will predict, given a review text whethe or not the rating is positive or non-positive.\n",
    "\n",
    "The steps taken during this notebook are the follows:<br>\n",
    "- Create a column to indicate if the score was positive or negative, this will be called positive_review\n",
    "- The text will be turned into vectors using keras implementation of word2vec\n",
    "- distribution of the vectors will be analyzed to check for outliers and discard them as necessary.\n",
    "- short vectors will be 0-padded to match in length to the longer vectors.\n",
    "- CNNs will be trained using different parameters and performance compared amongst the other CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#data importing/wrangling\n",
    "import pandas as pd\n",
    "\n",
    "#text processing \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#CNN modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, Conv1D, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "#keras functionsl api\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "#plotting CNN\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, Activation\n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "#data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "#only required first time\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "input_location = '/Users/momori/data/reviews_processed.csv'\n",
    "#for aws\n",
    "#input_location='/home/ubuntu/data/reviews_processed.csv'\n",
    "\n",
    "model_output = '/Users/momori/data/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(input_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create documents and labels to train the model later\n",
    "docs = data['Text']\n",
    "labels = data['positive_review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embedding\n",
    "\n",
    "Here we will use the word2vec embedding algorithm implementation by keras.  Since neural networks can only take in numerical values and vectors to train from, the texts will need to be encoded into numerical values.  After each review is encoded, the lengths of the review will be analyzed because the shorter review vedctors needs to be 0-padded to match the longest review vector.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 17, 125, 319, 7, 1, 4940, 523, 103, 52, 204, 3, 17, 117, 28, 41, 5, 30, 7, 29, 183, 1, 38, 629, 48, 26, 4, 2636, 58, 4, 1183, 448, 3, 6, 619, 99, 13, 5266, 8, 1777, 3, 94, 8695, 9, 38, 99, 58, 140]\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec embedding\n",
    "\n",
    "#use the tokenizer API provided by Keras to turn documents into sequences\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "#encode the documents into integers\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a list of lengths of each of the encoded_docs to find out the longest length, \n",
    "#and see if any 0 padding is required\n",
    "doc_lengths = [len(doc) for doc in encoded_docs]\n",
    "max_doc_length = max(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  percentile of document length: 57.0\n",
      "60  percentile of document length: 70.0\n",
      "70  percentile of document length: 88.0\n",
      "80  percentile of document length: 114.0\n",
      "90  percentile of document length: 164.0\n",
      "99  percentile of document length: 398.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMIIgA4ARBBkAjPgf6qs6AGZ2iOYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2fdff050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's put the lengths into a boxplot to see the distributions of the lengths\n",
    "np_array = np.array(doc_lengths)\n",
    "\n",
    "for i in [50,60,70,80,90,99]:\n",
    "    print i, \" percentile of document length:\", np.percentile(np_array,i)\n",
    "    \n",
    "ax = sns.boxplot(x=doc_lengths, showfliers=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE4WDiIiYKBxERMRE4SAiIiYKBxERMfl/Bn+HBYA2KHAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a321aa090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's put the lengths into a histogram to see the distributions of the lengths\n",
    "plt.hist([x for x in doc_lengths if x <= 200], bins=30)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cap the maximum length of the reviews to 200 words, as that will give us a good sense of the overall data without having to zero pad the short reviews to match really long reviews.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#zero pad the shorter texts\n",
    "max_length = 200\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872496\n"
     ]
    }
   ],
   "source": [
    "#find the vocab size from tokenizer\n",
    "vocab_size = t.word_counts[max(t.word_counts,key=t.word_counts.get)]\n",
    "print vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "now that we have the data, we can split this into train/test and create a CNN using Keras.\n",
    "The CNN will be layered as follows:\n",
    "\n",
    "    The embedding layer, which is the input layer consisting of the embedded word vectors.\n",
    "    A Flatten layer, to make the 2-dimensional input into a 1-dimensional output.\n",
    "    A Dense layer, which will create a fully connected layer with 1 node for the output.\n",
    "    activation function is sigmoid, because this will give us a binary output, either positive review or non-positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data\n",
    "train_size = 0.7\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    padded_docs, labels, test_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use smaller sample size\n",
    "train_size = 0.7\n",
    "samp_size = 0.3\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    padded_docs[:int(len(padded_docs)*samp_size)], labels[:int(len(padded_docs)*samp_size)]\\\n",
    "    , test_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119375"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 4)            7489984   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 1)            13        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,490,098\n",
      "Trainable params: 7,490,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create the CNN\n",
    "vect_dimension = 4 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "model.add(Conv1D(1, kernel_size=3, padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#model compilation. add loss function\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# check model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size is:  1\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 86s 3ms/step - loss: 0.5109 - acc: 0.7715\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 82s 2ms/step - loss: 0.3994 - acc: 0.8087\n",
      "34107/34107 [==============================] - 1s 40us/step\n",
      "Accuracy: 82.980033\n",
      "79583/79583 [==============================] - 3s 36us/step\n",
      "Accuracy: 80.100021\n",
      "window_size is:  2\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 85s 2ms/step - loss: 0.5011 - acc: 0.7736\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 83s 2ms/step - loss: 0.3806 - acc: 0.8194\n",
      "34107/34107 [==============================] - 2s 50us/step\n",
      "Accuracy: 86.199314\n",
      "79583/79583 [==============================] - 3s 43us/step\n",
      "Accuracy: 82.434691\n",
      "window_size is:  3\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 82s 2ms/step - loss: 0.5244 - acc: 0.7710\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 82s 2ms/step - loss: 0.3625 - acc: 0.8338\n",
      "34107/34107 [==============================] - 2s 45us/step\n",
      "Accuracy: 86.653766\n",
      "79583/79583 [==============================] - 3s 41us/step\n",
      "Accuracy: 82.697310\n",
      "window_size is:  4\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 84s 2ms/step - loss: 0.4949 - acc: 0.7751\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 88s 3ms/step - loss: 0.3656 - acc: 0.8231\n",
      "34107/34107 [==============================] - 2s 52us/step\n",
      "Accuracy: 87.310523\n",
      "79583/79583 [==============================] - 3s 44us/step\n",
      "Accuracy: 83.158463\n",
      "window_size is:  5\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 90s 3ms/step - loss: 0.4827 - acc: 0.7751\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 87s 3ms/step - loss: 0.3511 - acc: 0.8327\n",
      "34107/34107 [==============================] - 2s 51us/step\n",
      "Accuracy: 86.134811\n",
      "79583/79583 [==============================] - 4s 45us/step\n",
      "Accuracy: 82.414586\n",
      "window_size is:  6\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 92s 3ms/step - loss: 0.4775 - acc: 0.7776\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 91s 3ms/step - loss: 0.3345 - acc: 0.8489\n",
      "34107/34107 [==============================] - 2s 51us/step\n",
      "Accuracy: 89.242677\n",
      "79583/79583 [==============================] - 4s 45us/step\n",
      "Accuracy: 83.821922\n",
      "window_size is:  7\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 92s 3ms/step - loss: 0.4872 - acc: 0.7708\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 92s 3ms/step - loss: 0.3593 - acc: 0.8229\n",
      "34107/34107 [==============================] - 2s 52us/step\n",
      "Accuracy: 86.861934\n",
      "79583/79583 [==============================] - 4s 46us/step\n",
      "Accuracy: 83.427365\n",
      "window_size is:  8\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 92s 3ms/step - loss: 0.4695 - acc: 0.7785\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 95s 3ms/step - loss: 0.3361 - acc: 0.8431\n",
      "34107/34107 [==============================] - 2s 54us/step\n",
      "Accuracy: 88.650424\n",
      "79583/79583 [==============================] - 4s 47us/step\n",
      "Accuracy: 83.963912\n",
      "window_size is:  9\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 95s 3ms/step - loss: 0.4861 - acc: 0.7725\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 91s 3ms/step - loss: 0.3359 - acc: 0.8440\n",
      "34107/34107 [==============================] - 2s 52us/step\n",
      "Accuracy: 88.964142\n",
      "79583/79583 [==============================] - 4s 44us/step\n",
      "Accuracy: 83.831974\n",
      "window_size is:  10\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 93s 3ms/step - loss: 0.4769 - acc: 0.7751\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 93s 3ms/step - loss: 0.3348 - acc: 0.8458\n",
      "34107/34107 [==============================] - 2s 55us/step\n",
      "Accuracy: 88.225291\n",
      "79583/79583 [==============================] - 4s 46us/step\n",
      "Accuracy: 83.747785\n",
      "window_size is:  11\n",
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 91s 3ms/step - loss: 0.4654 - acc: 0.7795\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 90s 3ms/step - loss: 0.3254 - acc: 0.8514\n",
      "34107/34107 [==============================] - 2s 52us/step\n",
      "Accuracy: 87.419005\n",
      "79583/79583 [==============================] - 4s 45us/step\n",
      "Accuracy: 83.255218\n"
     ]
    }
   ],
   "source": [
    "#let's use the above neural network to find the optimal window size\n",
    "k_size = [i for i in range(1,12)]\n",
    "models = []\n",
    "for k in k_size:\n",
    "    print 'window_size is: ', k\n",
    "    #create the CNN\n",
    "    vect_dimension = 4 \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "    model.add(Conv1D(1, kernel_size=k, padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model jsing 50% of the data\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    #evaluate on test\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    model.save('model_kernel_size'+str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sensitivity analysis: window_size\n",
    "\n",
    "From the above cell, we've tried window size of 1-11. Here are the results:\n",
    "\n",
    "| window size | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      1      |         82.98       |      80.1        |\n",
    "|      2      |         86.22       |       82.43      |\n",
    "|      3      |        86.65        |       82.70      |\n",
    "|      4      |       87.31         |       83.15      |\n",
    "|      5      |      86.13          |       82.41      |\n",
    "|      6      |       89.24         |       83.82      |\n",
    "|      7      |        86.86        |       83.42      |\n",
    "|      8      |        88.65        |       83.96      |\n",
    "|      9      |        88.96        |       83.83      |\n",
    "|      10     |        88.22        |       83.74      |\n",
    "|      11     |        87.41        |        83.25     |\n",
    "\n",
    "We see the maximum accuracy for both training and testing when the window_size is 6. Hence we will try different number and combinations of filter size around 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Embedding(vocab_size, vect_dimension, input_length=max_length)\n",
    "# visible = Input(shape=(len(padded_docs[0]),))\n",
    "\n",
    "# #embedding\n",
    "# x = Embedding(vocab_size, vect_dimension, input_length=max_length)(visible)\n",
    "\n",
    "# #first feature extractor\n",
    "# conv1 = Conv1D(2, kernel_size = (6,), activation='relu'\\\n",
    "#                      ,input_shape=(len(x_train),))(x)\n",
    "\n",
    "# pool1 = MaxPooling1D(pool_size=4)(conv1)\n",
    "# flat1 = Flatten()(pool1)\n",
    "\n",
    "# #second feature extractor\n",
    "# conv2 = Conv1D(2, kernel_size=(6,), activation='relu'\\\n",
    "#               ,input_shape=(len(x_train),))(x)\n",
    "# pool2 = MaxPooling1D(pool_size=4)(conv2)\n",
    "# flat2 = Flatten()(pool2)\n",
    "\n",
    "# #concatenate the different feature extractors\n",
    "# merge = concatenate([flat1, flat2])\n",
    "\n",
    "# #add a dense layer\n",
    "# hidden1 = Dense(10, activation='relu')(merge)\n",
    "\n",
    "# #output layer\n",
    "# output = Dense(1, activation='sigmoid')(hidden1)\n",
    "# model=Model(inputs = visible, outputs = output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "34107/34107 [==============================] - 86s 3ms/step - loss: 0.4335 - acc: 0.8069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9b30da90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# model.fit(x_train,y_train)\n",
    "# loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79583/79583 [==============================] - 5s 61us/step\n",
      "Accuracy: 85.221718\n"
     ]
    }
   ],
   "source": [
    "# loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, <tf.Tensor 'flatten_6/Reshape:0' shape=(?, ?) dtype=float32>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create the models with different filter sizes\n",
    "vect_dimension = 4 \n",
    "\n",
    "filters = [[6],[6,6],[6,6,6],[5,6],[6,7],[5,6,7]]\n",
    "model_dictionary = {}\n",
    "for f in filters:\n",
    "    visible = Input(shape=(len(padded_docs[0]),))\n",
    "\n",
    "    #embedding\n",
    "    x = Embedding(vocab_size, vect_dimension, input_length=max_length)(visible)\n",
    "    \n",
    "    #for each filter in filters, we create a branch\n",
    "    if len(f) > 1:\n",
    "        num_filter = len(f)\n",
    "        convolutions = [None] * num_filter\n",
    "        pools = [None] * num_filter\n",
    "        flattens = [None] * num_filter\n",
    "        for index in range(len(f)):\n",
    "            convolutions[index] = Conv1D(2, kernel_size=(f[index],),\\\n",
    "                                        activation='relu',\\\n",
    "                                        input_shape=(len(x_train),))(x)\n",
    "            pools[index] = MaxPooling1D(pool_size=4)(convolutions[index])\n",
    "            flattens[index] = Flatten()(pools[index])\n",
    "\n",
    "        merge = concatenate(flattens)\n",
    "        \n",
    "    else: #only one filter\n",
    "        conv1 = Conv1D(2, kernel_size = (f[0],), activation='relu'\\\n",
    "                         ,input_shape=(len(x_train),))(x)\n",
    "        pool1 = MaxPooling1D(pool_size=4)(conv1)\n",
    "        merge = Flatten()(pool1)\n",
    "    \n",
    "    hidden1 = Dense(10, activation='relu')(merge)\n",
    "    output = Dense(1, activation='sigmoid')(hidden1)\n",
    "    model = Model(inputs = visible, outputs = output)\n",
    "    \n",
    "    #save model into dict\n",
    "    model_dictionary[model] = f\n",
    "    \n",
    "    plot_model(model, to_file=str(f)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using model with filter(s): [5, 6]\n",
      "51160/51160 [==============================] - 3s 67us/step\n",
      "training accuracy: 88.332682\n",
      "119376/119376 [==============================] - 8s 69us/step\n",
      "testing accuracy: 86.694143\n",
      "\n",
      "\n",
      "using model with filter(s): [6, 6, 6]\n",
      "51160/51160 [==============================] - 3s 60us/step\n",
      "training accuracy: 88.332682\n",
      "119376/119376 [==============================] - 7s 61us/step\n",
      "testing accuracy: 86.694143\n",
      "\n",
      "\n",
      "using model with filter(s): [6, 7]\n",
      "51160/51160 [==============================] - 3s 62us/step\n",
      "training accuracy: 88.332682\n",
      "119376/119376 [==============================] - 7s 62us/step\n",
      "testing accuracy: 86.694143\n",
      "\n",
      "\n",
      "using model with filter(s): [5, 6, 7]\n",
      "51160/51160 [==============================] - 4s 70us/step\n",
      "training accuracy: 91.872557\n",
      "119376/119376 [==============================] - 7s 62us/step\n",
      "testing accuracy: 88.048687\n",
      "\n",
      "\n",
      "using model with filter(s): [6]\n",
      "51160/51160 [==============================] - 3s 62us/step\n",
      "training accuracy: 91.872557\n",
      "119376/119376 [==============================] - 8s 63us/step\n",
      "testing accuracy: 88.048687\n",
      "\n",
      "\n",
      "using model with filter(s): [6, 6]\n",
      "51160/51160 [==============================] - 3s 63us/step\n",
      "training accuracy: 91.872557\n",
      "119376/119376 [==============================] - 8s 64us/step\n",
      "testing accuracy: 88.048687\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compile, fit and test the models\n",
    "for k, v in model_dictionary.iteritems():\n",
    "    print 'using model with filter(s):' , v\n",
    "    k.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    k.fit(x_train,y_train, verbose = 0)\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('training accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    #evaluate on test\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('testing accuracy: %f' % (accuracy*100))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different accuracies achieved by the above filters are below:\n",
    "\n",
    "| filters used | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      6     |         91.87       |      88.05        |\n",
    "|      6,6      |         91.87       |       88.05      |\n",
    "|      6,6,6      |        88.33        |       86.69      |\n",
    "|      5,6      |       88.33         |       86.69     |\n",
    "|      6,7      |      88.33          |       86.69      |\n",
    "|      5,6,7    |       91.87         |       88.05      |\n",
    "\n",
    "Due to the low number of samples used in training/testing, the neural network may have achieved its maximum fit with the above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis: Feature maps\n",
    "\n",
    "WIP:\n",
    "According to the article, feature maps should range between 100-600. We will spot check some of them and check the accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features is:  10\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 135s 3ms/step - loss: 0.3880 - acc: 0.8298\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 127s 2ms/step - loss: 0.2525 - acc: 0.8977\n",
      "51160/51160 [==============================] - 4s 71us/step\n",
      "Training Accuracy: 92.724785\n",
      "119376/119376 [==============================] - 7s 60us/step\n",
      "Testing Accuracy: 88.150885\n",
      "\n",
      "\n",
      "num_features is:  50\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 131s 3ms/step - loss: 0.3473 - acc: 0.8519\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 128s 3ms/step - loss: 0.2263 - acc: 0.9118\n",
      "51160/51160 [==============================] - 4s 76us/step\n",
      "Training Accuracy: 94.100860\n",
      "119376/119376 [==============================] - 8s 66us/step\n",
      "Testing Accuracy: 89.037998\n",
      "\n",
      "\n",
      "num_features is:  100\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 133s 3ms/step - loss: 0.3441 - acc: 0.8541\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 131s 3ms/step - loss: 0.2223 - acc: 0.9133\n",
      "51160/51160 [==============================] - 5s 90us/step\n",
      "Training Accuracy: 94.210321\n",
      "119376/119376 [==============================] - 10s 81us/step\n",
      "Testing Accuracy: 88.908993\n",
      "\n",
      "\n",
      "num_features is:  200\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 142s 3ms/step - loss: 0.3376 - acc: 0.8590\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 2700s 53ms/step - loss: 0.2225 - acc: 0.9126\n",
      "51160/51160 [==============================] - 8s 150us/step\n",
      "Training Accuracy: 94.337373\n",
      "119376/119376 [==============================] - 16s 137us/step\n",
      "Testing Accuracy: 89.234855\n",
      "\n",
      "\n",
      "num_features is:  400\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 179s 4ms/step - loss: 0.3304 - acc: 0.8602\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 175s 3ms/step - loss: 0.2179 - acc: 0.9158\n",
      "51160/51160 [==============================] - 15s 296us/step\n",
      "Training Accuracy: 94.757623\n",
      "119376/119376 [==============================] - 34s 282us/step\n",
      "Testing Accuracy: 89.419146\n",
      "\n",
      "\n",
      "num_features is:  600\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 180s 4ms/step - loss: 0.3332 - acc: 0.8607\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 176s 3ms/step - loss: 0.2181 - acc: 0.9145\n",
      "51160/51160 [==============================] - 22s 436us/step\n",
      "Training Accuracy: 94.689210\n",
      "119376/119376 [==============================] - 51s 425us/step\n",
      "Testing Accuracy: 89.398204\n",
      "\n",
      "\n",
      "num_features is:  800\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 209s 4ms/step - loss: 0.3245 - acc: 0.8633\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 190s 4ms/step - loss: 0.2090 - acc: 0.9194\n",
      "51160/51160 [==============================] - 28s 552us/step\n",
      "Training Accuracy: 95.048866\n",
      "119376/119376 [==============================] - 64s 537us/step\n",
      "Testing Accuracy: 89.558203\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's use the above neural network to find the optimal window size\n",
    "feature_maps = [10,50,100,200,400,600,800]\n",
    "models = []\n",
    "\n",
    "for feature in feature_maps:\n",
    "    print 'num_features is: ', feature\n",
    "    #create the CNN\n",
    "    vect_dimension = 4 \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "    model.add(Conv1D(feature, kernel_size=6, padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model jsing 50% of the data\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    #evaluate on test\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Testing Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    model.save('model_feature_map_'+str(k))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features is:  700\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 185s 4ms/step - loss: 0.3301 - acc: 0.8623\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 179s 4ms/step - loss: 0.2219 - acc: 0.9126\n",
      "51160/51160 [==============================] - 26s 500us/step\n",
      "Training Accuracy: 94.419468\n",
      "119376/119376 [==============================] - 56s 468us/step\n",
      "Testing Accuracy: 88.819361\n",
      "\n",
      "\n",
      "num_features is:  900\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 200s 4ms/step - loss: 0.3327 - acc: 0.8602\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 198s 4ms/step - loss: 0.2177 - acc: 0.9157\n",
      "51160/51160 [==============================] - 30s 577us/step\n",
      "Training Accuracy: 94.569977\n",
      "119376/119376 [==============================] - 67s 560us/step\n",
      "Testing Accuracy: 89.378099\n",
      "\n",
      "\n",
      "num_features is:  1000\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 217s 4ms/step - loss: 0.3322 - acc: 0.8613\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 216s 4ms/step - loss: 0.2215 - acc: 0.9145\n",
      "51160/51160 [==============================] - 33s 636us/step\n",
      "Training Accuracy: 92.931978\n",
      "119376/119376 [==============================] - 76s 640us/step\n",
      "Testing Accuracy: 87.621465\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's use the above neural network to find the optimal window size\n",
    "feature_maps = [700,900,1000]\n",
    "models = []\n",
    "\n",
    "for feature in feature_maps:\n",
    "    print 'num_features is: ', feature\n",
    "    #create the CNN\n",
    "    vect_dimension = 4 \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "    model.add(Conv1D(feature, kernel_size=6, padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model jsing 50% of the data\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    #evaluate on test\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Testing Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    model.save('model_feature_map_'+str(k))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP:<br>\n",
    "we see from the below table that we achieve the highest testing accuracy with 800 feature maps per convolution.\n",
    "\n",
    "| number of features | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      10    |         92.72      |      88.15        |\n",
    "|      50     |         94.10     |       89.03      |\n",
    "|      100      |        94.21        |       88.91      |\n",
    "|      200      |       94.33         |       89.23    |\n",
    "|      400      |      94.75         |       89.41|\n",
    "|      600    |       94.68         |       89.39      |\n",
    "|      700    |       94.41         |       88.82      |\n",
    "|      800    |       95.05         |       89.55      |\n",
    "|      900    |       94.57         |       89.37      |\n",
    "|      1000    |       92.93         |      87.62      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activation functions\n",
    "\n",
    "According to the article, RELU and TANH is the best suited functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation function is:  relu\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 201s 4ms/step - loss: 0.3350 - acc: 0.8589\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 208s 4ms/step - loss: 0.2192 - acc: 0.9145\n",
      "51160/51160 [==============================] - 28s 554us/step\n",
      "Training Accuracy: 93.960125\n",
      "119376/119376 [==============================] - 64s 539us/step\n",
      "Testing Accuracy: 88.866271\n",
      "\n",
      "\n",
      "activation function is:  tanh\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 206s 4ms/step - loss: 0.3438 - acc: 0.8539\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 204s 4ms/step - loss: 0.2372 - acc: 0.9065\n",
      "51160/51160 [==============================] - 34s 669us/step\n",
      "Training Accuracy: 94.317826\n",
      "119376/119376 [==============================] - 71s 598us/step\n",
      "Testing Accuracy: 88.165125\n",
      "\n",
      "\n",
      "activation function is:  sigmoid\n",
      "Epoch 1/2\n",
      "51160/51160 [==============================] - 207s 4ms/step - loss: 3.5416 - acc: 0.7778\n",
      "Epoch 2/2\n",
      "51160/51160 [==============================] - 210s 4ms/step - loss: 3.5422 - acc: 0.7778\n",
      "51160/51160 [==============================] - 33s 642us/step\n",
      "Training Accuracy: 77.781470\n",
      "119376/119376 [==============================] - 75s 627us/step\n",
      "Testing Accuracy: 77.623643\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's use the above neural network to find the optimal window size\n",
    "activation_functions = ['relu','tanh', 'sigmoid']\n",
    "models = []\n",
    "\n",
    "for func in activation_functions:\n",
    "    print 'activation function is: ', func\n",
    "    #create the CNN\n",
    "    vect_dimension = 4 \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "    model.add(Conv1D(800, kernel_size=6, padding='same',activation=func))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model jsing 50% of the data\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    #evaluate on test\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Testing Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    model.save('model_feature_map_'+str(k))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| activation function | training accuracy   | testing accuracy |\n",
    "|-------------|---------------------|------------------|\n",
    "|      relu    |         93.96      |      88.87        |\n",
    "|      tanh     |         94.50     |       87.66      |\n",
    "|      sigmoid     |         94.50     |       87.66      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "34107/34107 [==============================] - 81s 2ms/step - loss: 0.4366 - acc: 0.7876\n",
      "Epoch 2/2\n",
      "34107/34107 [==============================] - 84s 2ms/step - loss: 0.3402 - acc: 0.8446\n",
      "34107/34107 [==============================] - 1s 38us/step\n",
      "Accuracy: 87.375026\n"
     ]
    }
   ],
   "source": [
    "# fit the model jsing 50% of the data\n",
    "model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "#evaluate on test\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 1927s 11ms/step - loss: 0.3021 - acc: 0.8730\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 486s 3ms/step - loss: 0.2207 - acc: 0.9134\n",
      "170536/170536 [==============================] - 9s 54us/step\n",
      "Accuracy: 93.032556\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397918/397918 [==============================] - 22s 56us/step\n",
      "Accuracy: 90.390482\n"
     ]
    }
   ],
   "source": [
    "#put on test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 4)            7489984   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               42000     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 7,532,085\n",
      "Trainable params: 7,532,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create RNN\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(vocab_size, vect_dimension, input_length=max_length))\n",
    "rnn_model.add(LSTM(100))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(rnn_model.summary())\n",
    "#model.fit(x_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 1241s 7ms/step - loss: 0.5254 - acc: 0.7805\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 1205s 7ms/step - loss: 0.5066 - acc: 0.7815\n",
      "170536/170536 [==============================] - 8s 50us/step\n",
      "Accuracy: 77.900267\n"
     ]
    }
   ],
   "source": [
    "rnn_model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282400/397918 [====================>.........] - ETA: 1:49"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6a62db98ec05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#put on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m    920\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1690\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m                                steps=steps)\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#put on test set\n",
    "loss, accuracy = rnn_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Here, we will try different parameters to tune the CNN and check the performance. \n",
    "The first cell will focus on how many dimensions to use and the second cell will focus on different optimizers/loss functions and the result will be plotted in a heatmap to visualize the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dim:  softmax\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 448s 3ms/step - loss: 3.5128 - acc: 0.7797\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 471s 3ms/step - loss: 3.5128 - acc: 0.7797\n",
      "170536/170536 [==============================] - 4s 26us/step\n",
      "Training Accuracy: 77.965356\n",
      "397918/397918 [==============================] - 11s 26us/step\n",
      "Test Accuracy: 78.111068\n",
      "\n",
      "\n",
      "dim:  relu\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 450s 3ms/step - loss: 0.4290 - acc: 0.8162\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 448s 3ms/step - loss: 0.3487 - acc: 0.8478\n",
      "170536/170536 [==============================] - 5s 28us/step\n",
      "Training Accuracy: 78.470235\n",
      "397918/397918 [==============================] - 11s 28us/step\n",
      "Test Accuracy: 75.204188\n",
      "\n",
      "\n",
      "dim:  tanh\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 439s 3ms/step - loss: 0.4380 - acc: 0.8170\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 426s 3ms/step - loss: 0.3396 - acc: 0.8584\n",
      "170536/170536 [==============================] - 5s 27us/step\n",
      "Training Accuracy: 87.092461\n",
      "397918/397918 [==============================] - 11s 27us/step\n",
      "Test Accuracy: 85.298981\n",
      "\n",
      "\n",
      "dim:  sigmoid\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 449s 3ms/step - loss: 0.3112 - acc: 0.8682\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 432s 3ms/step - loss: 0.2351 - acc: 0.9068\n",
      "170536/170536 [==============================] - 5s 27us/step\n",
      "Training Accuracy: 92.204579\n",
      "397918/397918 [==============================] - 11s 27us/step\n",
      "Test Accuracy: 89.890882\n",
      "\n",
      "\n",
      "dim:  linear\n",
      "Epoch 1/2\n",
      "170536/170536 [==============================] - 430s 3ms/step - loss: 0.4437 - acc: 0.8170\n",
      "Epoch 2/2\n",
      "170536/170536 [==============================] - 436s 3ms/step - loss: 0.3315 - acc: 0.8427\n",
      "170536/170536 [==============================] - 5s 27us/step\n",
      "Training Accuracy: 82.598396\n",
      "397918/397918 [==============================] - 11s 27us/step\n",
      "Test Accuracy: 80.199438\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#different activation functions\n",
    "#create multiple CNNs with different hyperparameters\n",
    "\n",
    "activation_functions = ['softmax', 'relu', 'tanh', 'sigmoid', 'linear']\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "for af in activation_functions:\n",
    "    print 'dim: ', af\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 4, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=af))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "    #put on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    models_accuracy[model]['training_error'] = train_accuracy\n",
    "    models_accuracy[model]['test_error']=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 4, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create multiple CNNs with different hyperparameters\n",
    "\n",
    "vect_dimensions = [2,4,6,8]\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "for dim in vect_dimensions:\n",
    "    print 'dim: ', dim\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, dim, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model compilation. add loss function\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print('Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "    #put on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    models_accuracy[model]['training_error'] = train_accuracy\n",
    "    models_accuracy[model]['test_error']=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the mdoels which was just created\n",
    "post_name = 1\n",
    "for k,v in models_accuracy.iteritems():\n",
    "    out_name = 'keras_model_'+str(post_name)\n",
    "    k.save(out_name)\n",
    "    post_name+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizers = ['adam', 'sgd', 'Adagrad', 'Adadelta']\n",
    "losses = ['mean_squared_error', 'cosine_proximity', 'binary_crossentropy']\n",
    "metrics=['acc']\n",
    "\n",
    "models_accuracy = defaultdict(dict)\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for loss in losses:\n",
    "        print 'optimizer: ', optimizer\n",
    "        print 'loss: ', loss, '\\n'\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, 2, input_length=max_length))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        #model compilation. add loss function\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "        # fit the model\n",
    "        model.fit(x_train, y_train, epochs=2, verbose=1)\n",
    "\n",
    "        # evaluate the model\n",
    "        train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "        print('\\n Training Accuracy: %f' % (train_accuracy*100))\n",
    "\n",
    "        #put on test set\n",
    "        test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "        print('\\n Test Accuracy: %f' % (test_accuracy*100))\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        #models_accuracy[model]['training_error'] = train_accuracy\n",
    "        #models_accuracy[model]['test_error']=test_accuracy\n",
    "        models_accuracy[optimizer][loss] = defaultdict(dict)\n",
    "        models_accuracy[optimizer][loss]['train_acc']=train_accuracy\n",
    "        models_accuracy[optimizer][loss]['test_acc']=test_accuracy\n",
    "\n",
    "        \n",
    "        print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('hyper_parameter_tuning.pickle', 'wb') as handle:\n",
    "    pickle.dump(models_accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('hyper_parameter_tuning.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print models_accuracy == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
