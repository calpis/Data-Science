{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CNN and dqn generation\n",
    "\n",
    "\n",
    "https://www.intelnervana.com/demystifying-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from classes import StackedImages, ImageProcessor\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import timeit\n",
    "\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#down-sized dimensions of the state representations\n",
    "input_num_rows = 96\n",
    "input_num_cols = 114\n",
    "\n",
    "data_input = '/users/momori/data/stacked_images.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing with CNN (Convolutional Neural Network)\n",
    "\n",
    "input shape is (4,288, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''simple convolutional neural network'''\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(6, (8,8), activation='relu', input_shape=(288, 512, 4)))\n",
    "# model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_reward(value):\n",
    "    if value==0.0:\n",
    "        return 0.001\n",
    "    elif value==5.0:\n",
    "        return 20.0\n",
    "    elif value==-5.0:\n",
    "        return -5.0\n",
    "\n",
    "def test_game(agent):\n",
    "    #init the game flappybird\n",
    "    game = FlappyBird()\n",
    "\n",
    "    #set up the screen\n",
    "    p = PLE(game, fps=30, display_screen=True)\n",
    "\n",
    "    p.init()\n",
    "\n",
    "    observations = deque()\n",
    "\n",
    "    max_frames = 100 ##first frame is null\n",
    "    num_frame_stacks = 4\n",
    "    num_to_observe = 12 #observations before we train the network\n",
    "    batch_count = 4 #train the model by selecting some subsample of the stored replays\n",
    "                    #contains lists of replay envs. each element is in the format\n",
    "                    #[state, action, reward, new_state, terminal(boolean)]\n",
    "    epsilon = 0.8 #when to choose the known best action vs random action\n",
    "    gamma = 0.8 #importance of future rewards compared to current reward\n",
    "    \n",
    "    total_reward = 0\n",
    "\n",
    "    replay_data = deque() \n",
    "\n",
    "    game_over = False\n",
    "    for i in range(0, max_frames):\n",
    "        observation = p.getScreenRGB()\n",
    "        if np.max(observation) != 0:\n",
    "            observations.append(observation)\n",
    "\n",
    "\n",
    "        ##after num_frame_stacks frames pass, we start predicting actions\n",
    "        if len(observations) == num_frame_stacks:\n",
    "            image_processors = [ImageProcessor(observations[j])\n",
    "                               for j in range(num_frame_stacks)]\n",
    "            stacked_images = StackedImages(image_processors, num_frame_stacks)\n",
    "            action, output = agent.predict(stacked_images.get_stacked_images())\n",
    "            reward = update_reward(p.act(action))\n",
    "            total_reward += reward\n",
    "            #print action, output\n",
    "\n",
    "            #get new state and create the replay data\n",
    "            observation = p.getScreenRGB()\n",
    "#             replay_data.append([stacked_images, action, reward, \n",
    "#                                 ImageProcessor(observation),p.game_over()])\n",
    "\n",
    "            #remove the oldest state so new state can be added in next iteration\n",
    "            observations.popleft()\n",
    "\n",
    "        else: ##first few frames, do nothing\n",
    "            p.act(None)\n",
    "\n",
    "        #if game over, reset the game and the observations queue\n",
    "        if p.game_over():\n",
    "            p.reset_game()\n",
    "            observations = deque()\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.PolicyAgent instance at 0x1c3333de60>\n",
      "119 [[ 0.1393802  0.       ]]\n",
      "119 [[ 0.13503227  0.        ]]\n",
      "119 [[ 0.12506482  0.        ]]\n",
      "119 [[ 0.14631693  0.        ]]\n",
      "119 [[ 0.14587004  0.        ]]\n",
      "119 [[ 0.13164666  0.        ]]\n",
      "119 [[ 0.11812669  0.        ]]\n",
      "119 [[ 0.12161004  0.        ]]\n",
      "119 [[ 0.14044766  0.        ]]\n",
      "119 [[ 0.12986581  0.        ]]\n",
      "119 [[ 0.14485033  0.        ]]\n",
      "119 [[ 0.14546552  0.        ]]\n",
      "119 [[ 0.10938943  0.        ]]\n",
      "119 [[ 0.1036599  0.       ]]\n",
      "119 [[ 0.12555911  0.        ]]\n",
      "119 [[ 0.12564251  0.        ]]\n",
      "119 [[ 0.1291144  0.       ]]\n",
      "119 [[ 0.14856569  0.        ]]\n",
      "119 [[ 0.13094872  0.        ]]\n",
      "119 [[ 0.14893465  0.        ]]\n",
      "119 [[ 0.16122477  0.        ]]\n",
      "119 [[ 0.15574256  0.        ]]\n",
      "119 [[ 0.17841849  0.        ]]\n",
      "119 [[ 0.18821411  0.        ]]\n",
      "119 [[ 0.17314672  0.        ]]\n",
      "119 [[ 0.19936012  0.        ]]\n",
      "119 [[ 0.21840173  0.        ]]\n",
      "119 [[ 0.20229319  0.        ]]\n",
      "119 [[ 0.18935464  0.        ]]\n",
      "119 [[ 0.18822902  0.        ]]\n",
      "119 [[ 0.1675408  0.       ]]\n",
      "119 [[ 0.16895503  0.        ]]\n",
      "119 [[ 0.1931622  0.       ]]\n",
      "119 [[ 0.17252767  0.        ]]\n",
      "119 [[ 0.18537749  0.        ]]\n",
      "119 [[ 0.19766125  0.        ]]\n",
      "119 [[ 0.17358182  0.        ]]\n",
      "119 [[ 0.19506033  0.        ]]\n",
      "119 [[ 0.19445688  0.        ]]\n",
      "119 [[ 0.18778215  0.        ]]\n",
      "119 [[ 0.20068522  0.        ]]\n",
      "119 [[ 0.20127133  0.        ]]\n",
      "119 [[ 0.1830736  0.       ]]\n",
      "119 [[ 0.20281912  0.        ]]\n",
      "119 [[ 0.19678161  0.        ]]\n",
      "119 [[ 0.16150181  0.        ]]\n",
      "119 [[ 0.19807276  0.        ]]\n",
      "119 [[ 0.1939086  0.       ]]\n",
      "119 [[ 0.18341166  0.        ]]\n",
      "119 [[ 0.17825921  0.        ]]\n",
      "119 [[ 0.19113478  0.        ]]\n",
      "119 [[ 0.1747946  0.       ]]\n",
      "119 [[ 0.19466709  0.        ]]\n",
      "119 [[ 0.19880836  0.        ]]\n",
      "119 [[ 0.16683614  0.        ]]\n",
      "119 [[ 0.18689859  0.        ]]\n",
      "119 [[ 0.19122255  0.        ]]\n",
      "119 [[ 0.16188422  0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_game(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyAgent():\n",
    "    '''agent to learn and provide optimal actions during the game run.\n",
    "    the output is the expected reward for taking that action in that state\n",
    "    output:\n",
    "        higher index 0 -> flap\n",
    "        higher index 1 -> do nothing\n",
    "        '''\n",
    "    def __init__(self):\n",
    "        ##cnn \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8,8), activation='relu', input_shape=(input_num_rows, input_num_cols, 4)))\n",
    "        model.add(MaxPooling2D(pool_size=(10,10)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        #model.add(Dense(2, activation='softmax'))\n",
    "        model.add(Dense(2, activation='relu'))\n",
    "        #model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "        model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "        self._model = model\n",
    "    \n",
    "    \n",
    "    def fit(self, x_data, y_data, batch_size=32, epochs=1):\n",
    "        x_data = x_data.reshape(1, x_data.shape[1], x_data.shape[2], x_data.shape[0])\n",
    "        y_data = np.array(y_data).reshape(1,2)\n",
    "        self._model.fit(x_data, y_data, batch_size, epochs, verbose=0)\n",
    "        \n",
    "    def predict(self, x_data):\n",
    "        x_data = x_data.reshape(1, x_data.shape[1], x_data.shape[2], x_data.shape[0])\n",
    "        output = self._model.predict(x_data)\n",
    "        index = np.random.choice(np.flatnonzero(output==np.max(output)))\n",
    "#         if output[0][0] > output[0][1]:\n",
    "#             action = 119\n",
    "#         else:\n",
    "#             action = None\n",
    "        if index==0:\n",
    "            action = 119\n",
    "        else:\n",
    "            action = None\n",
    "        return action, output\n",
    "    \n",
    "    def train_on_batch(self, x_data, y_data):\n",
    "        #x_data = x_data.reshape(1, x_data.shape[1], x_data.shape[2], x_data.shape[0])\n",
    "        self._model.train_on_batch(x_data, y_data)\n",
    "        \n",
    "    def summary(self):\n",
    "        return self._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_policy(agent, replay_data, num_frame_stacks=4, batch_count=0):\n",
    "    '''use the replay data to train the network\n",
    "    agent: PolicyAgent object\n",
    "    replay_data: queue of lists\n",
    "                 [0]: StackedImage object (current state)\n",
    "                 [1]: Action\n",
    "                 [2]: Reward\n",
    "                 [3]: ImageProcessor object (next state)\n",
    "                 [4]: is game terminal (boolean)\n",
    "    num_frame_stack: number of frames to stack as one data sample\n",
    "    batch_count: non-zero value indicates to use experience replay. \n",
    "                 # of frames to use from this replay_data to train the network\n",
    "    '''\n",
    "    #if batch_count is not 0, we use random sampling of the replay data\n",
    "    if batch_count != 0:\n",
    "        replay_data = random.sample(replay_data, batch_count)\n",
    "\n",
    "    #inputs/targets for training the neural network\n",
    "    #inputs = []\n",
    "    #targets = []\n",
    "\n",
    "    for i in range(0,len(replay_data),num_frame_stacks):\n",
    "        curr_state = replay_data[i][0].get_stacked_images()\n",
    "\n",
    "        #modified later to keep track of next stacked frames\n",
    "        next_state = np.split(curr_state, num_frame_stacks, axis=0)\n",
    "\n",
    "        action = replay_data[i][1]\n",
    "        if action == 119:  #change to proper index of the action output\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "        reward = replay_data[i][2]\n",
    "        next_frame = replay_data[i][3].gray_image\\\n",
    "            .reshape(1, input_num_rows, input_num_cols) #reshape to  stack properly onto the current state\n",
    "\n",
    "        #append latest state and pop the earliest\n",
    "        next_state.append(next_frame)\n",
    "        next_state.pop(0)\n",
    "\n",
    "        is_terminal = replay_data[i][4]\n",
    "\n",
    "        #get the highest reward possible in the next state\n",
    "        _, output = agent.predict(np.concatenate(next_state, axis=0))\n",
    "        next_reward = np.max(output)\n",
    "\n",
    "        #if terminal, no more rewards\n",
    "        if is_terminal:\n",
    "            total_reward = -5\n",
    "        else:\n",
    "            total_reward = reward + gamma * next_reward\n",
    "\n",
    "        #inputs.append(curr_state)\n",
    "        #targets.append(output[0])\n",
    "        output[0][action] = total_reward\n",
    "\n",
    "        agent.fit(curr_state, output[0])\n",
    "        \n",
    "    return agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "output_file = 'logs/log'\n",
    "#init the game flappybird\n",
    "game = FlappyBird()\n",
    "\n",
    "#set up the screen\n",
    "p = PLE(game, fps=30, display_screen=True)\n",
    "\n",
    "p.init()\n",
    "\n",
    "agent = PolicyAgent()\n",
    "observations = deque()\n",
    "\n",
    "'''values to change in AWS env'''\n",
    "max_frames = 105#640000 ##first frame is null\n",
    "num_frame_stacks = 4\n",
    "num_to_observe = 20#3200 #observations before we train the network\n",
    "\n",
    "\n",
    "batch_count = int(num_to_observe * 0.2)  #train the model by selecting some subsample of the stored replays\n",
    "                #contains lists of replay envs. each element is in the format\n",
    "                #[state, action, reward, new_state, terminal(boolean)]\n",
    "epsilon = 0.7 #choose best action 80% of the time\n",
    "gamma = 0.8 #importance of future rewards compared to current reward\n",
    "train_after_batch = True\n",
    "count_game_over = 0\n",
    "train_iteration_scores = []\n",
    "\n",
    "replay_data = deque() \n",
    "\n",
    "game_over = False\n",
    "\n",
    "#start time\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(0, max_frames):\n",
    "    if i % (max_frames/20) == 0:\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        with open(output_file, 'a') as log_handle:\n",
    "            log_handle.write('done with ' + str(i) + ' frames with ' + str(elapsed) + ' seconds\\n')\n",
    "        \n",
    "        #reset timer\n",
    "        start_time = timeit.default_timer()\n",
    "    \n",
    "    observation = p.getScreenRGB()\n",
    "    if np.max(observation) != 0:\n",
    "        observations.append(observation)\n",
    "    \n",
    "    \n",
    "    ##after num_frame_stacks frames pass, we start predicting actions\n",
    "    if len(observations) == num_frame_stacks:\n",
    "        image_processors = [ImageProcessor(observations[j])\n",
    "                           for j in range(num_frame_stacks)]\n",
    "        stacked_images = StackedImages(image_processors, num_frame_stacks)\n",
    "        if random.random() < epsilon:\n",
    "            action, output = agent.predict(stacked_images.get_stacked_images())\n",
    "        else:\n",
    "            action = np.random.choice([119, None])\n",
    "\n",
    "        ##update rewards    \n",
    "        reward = update_reward(p.act(action))\n",
    "        \n",
    "                \n",
    "        #get new state and create the replay data\n",
    "        observation = p.getScreenRGB()\n",
    "        replay_data.append([stacked_images, action, reward, \n",
    "                            ImageProcessor(observation),p.game_over()])\n",
    "        \n",
    "        #remove the oldest state\n",
    "        observations.popleft()\n",
    "        \n",
    "    else: ##first few frames, do nothing\n",
    "        p.act(None)\n",
    "    \n",
    "    #if game over, reset the game and the observations queue\n",
    "    if p.game_over():\n",
    "        count_game_over += 1\n",
    "        p.reset_game()\n",
    "        observations = deque()\n",
    "        #print 'game reset'\n",
    "        \n",
    "    #once we have enough data, train on a subsample of it\n",
    "    if len(replay_data) == num_to_observe:\n",
    "        agent = train_policy(agent, replay_data, num_frame_stacks=num_frame_stacks, batch_count=batch_count)\n",
    "        \n",
    "        if train_after_batch:\n",
    "            test_score = test_game(agent)\n",
    "            train_iteration_scores.append([count_game_over, test_score]) \n",
    "            \n",
    "        ##reset\n",
    "        replay_data = deque()\n",
    "\n",
    "with open(str(train_after_batch)+'_train_iteration_scores.pkl', 'w') as f:\n",
    "    pickle.dump(train_iteration_scores,f)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.PolicyAgent instance at 0x1c7dfb4248>\n",
      "119 [[ 1.49485052  0.        ]]\n",
      "119 [[ 1.50090814  0.        ]]\n",
      "119 [[ 1.50800264  0.        ]]\n",
      "119 [[ 1.5117799  0.       ]]\n",
      "119 [[ 1.50597656  0.        ]]\n",
      "119 [[ 1.52500486  0.        ]]\n",
      "119 [[ 1.51873565  0.        ]]\n",
      "119 [[ 1.5251081  0.       ]]\n",
      "119 [[ 1.55243731  0.        ]]\n",
      "119 [[ 1.55760431  0.        ]]\n",
      "119 [[ 1.56204641  0.        ]]\n",
      "119 [[ 1.5768944  0.       ]]\n",
      "119 [[ 1.58816552  0.        ]]\n",
      "119 [[ 1.60141385  0.        ]]\n",
      "119 [[ 1.62619567  0.        ]]\n",
      "119 [[ 1.6374954  0.       ]]\n",
      "119 [[ 1.64497173  0.        ]]\n",
      "119 [[ 1.66156971  0.        ]]\n",
      "119 [[ 1.65528584  0.        ]]\n",
      "119 [[ 1.66183329  0.        ]]\n",
      "119 [[ 1.67012346  0.        ]]\n",
      "119 [[ 1.65949166  0.        ]]\n",
      "119 [[ 1.66899228  0.        ]]\n",
      "119 [[ 1.68996084  0.        ]]\n",
      "119 [[ 1.68270075  0.        ]]\n",
      "119 [[ 1.68256164  0.        ]]\n",
      "119 [[ 1.6801883  0.       ]]\n",
      "119 [[ 1.66394174  0.        ]]\n",
      "119 [[ 1.66512096  0.        ]]\n",
      "119 [[ 1.67218661  0.        ]]\n",
      "119 [[ 1.66065311  0.        ]]\n",
      "119 [[ 1.65745008  0.        ]]\n",
      "119 [[ 1.65818632  0.        ]]\n",
      "119 [[ 1.66148937  0.        ]]\n",
      "119 [[ 1.67187142  0.        ]]\n",
      "119 [[ 1.67946327  0.        ]]\n",
      "119 [[ 1.68800688  0.        ]]\n",
      "119 [[ 1.68504393  0.        ]]\n",
      "119 [[ 1.67268825  0.        ]]\n",
      "119 [[ 1.67334831  0.        ]]\n",
      "119 [[ 1.68137014  0.        ]]\n",
      "119 [[ 1.67375624  0.        ]]\n",
      "119 [[ 1.67460907  0.        ]]\n",
      "119 [[ 1.68499422  0.        ]]\n",
      "119 [[ 1.67754948  0.        ]]\n",
      "119 [[ 1.68143809  0.        ]]\n",
      "119 [[ 1.70112836  0.        ]]\n",
      "119 [[ 1.70141125  0.        ]]\n",
      "119 [[ 1.71389985  0.        ]]\n",
      "119 [[ 1.72843528  0.        ]]\n",
      "119 [[ 1.73016942  0.        ]]\n",
      "119 [[ 1.74678171  0.        ]]\n",
      "119 [[ 1.76531422  0.        ]]\n",
      "119 [[ 1.76403737  0.        ]]\n",
      "119 [[ 1.75461721  0.        ]]\n",
      "119 [[ 1.75362444  0.        ]]\n",
      "119 [[ 1.75954568  0.        ]]\n",
      "119 [[ 1.74067986  0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52.0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_game(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 52.0], [0, 52.0], [1, 52.0], [1, 52.0]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('train_iteration_scores.pkl', 'w') as f:\n",
    "    pickle.dump(train_iteration_scores, f)\n",
    "\n",
    "train_iteration_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_iteration_scores.pkl', 'r') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 52.0], [0, 52.0], [1, 52.0], [1, 52.0]]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 game reset\n",
      "game reset\n",
      "game reset\n",
      "200 game reset\n",
      "game reset\n",
      "game reset\n",
      "400 game reset\n",
      "game reset\n",
      "game reset\n",
      "game reset\n",
      "600 game reset\n",
      "game reset\n",
      "game reset\n",
      "800 game reset\n",
      "game reset\n",
      "game reset\n",
      "game reset\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "##BACKUP\n",
    "\n",
    "#%%time\n",
    "#init the game flappybird\n",
    "game = FlappyBird()\n",
    "\n",
    "#set up the screen\n",
    "p = PLE(game, fps=30, display_screen=True)\n",
    "\n",
    "p.init()\n",
    "\n",
    "agent = PolicyAgent()\n",
    "observations = deque()\n",
    "\n",
    "max_frames = 640000 ##first frame is null\n",
    "num_frame_stacks = 4\n",
    "num_to_observe = 3200 #observations before we train the network\n",
    "batch_count = num_to_observe * 0.2  #train the model by selecting some subsample of the stored replays\n",
    "                #contains lists of replay envs. each element is in the format\n",
    "                #[state, action, reward, new_state, terminal(boolean)]\n",
    "epsilon = 0.8 #when to choose the known best action vs random action\n",
    "gamma = 0.8 #importance of future rewards compared to current reward\n",
    "\n",
    "replay_data = deque() \n",
    "\n",
    "game_over = False\n",
    "for i in range(0, max_frames):\n",
    "    if i % 200 == 0:\n",
    "        print i,\n",
    "    observation = p.getScreenRGB()\n",
    "    if np.max(observation) != 0:\n",
    "        observations.append(observation)\n",
    "    \n",
    "    \n",
    "    ##after num_frame_stacks frames pass, we start predicting actions\n",
    "    if len(observations) == num_frame_stacks:\n",
    "        image_processors = [ImageProcessor(observations[j])\n",
    "                           for j in range(num_frame_stacks)]\n",
    "        stacked_images = StackedImages(image_processors, num_frame_stacks)\n",
    "        if random.random() < epsilon:\n",
    "            action, output = agent.predict(stacked_images.get_stacked_images())\n",
    "        else:\n",
    "            action = np.random.choice([119, None])\n",
    "\n",
    "        ##update rewards    \n",
    "        reward = update_reward(p.act(action))\n",
    "        \n",
    "        \n",
    "        #print action, output\n",
    "        \n",
    "        #get new state and create the replay data\n",
    "        observation = p.getScreenRGB()\n",
    "        replay_data.append([stacked_images, action, reward, \n",
    "                            ImageProcessor(observation),p.game_over()])\n",
    "        \n",
    "        #remove the oldest state\n",
    "        observations.popleft()\n",
    "        \n",
    "    else: ##first few frames, do nothing\n",
    "        p.act(None)\n",
    "    \n",
    "    #if game over, reset the game and the observations queue\n",
    "    if p.game_over():\n",
    "        p.reset_game()\n",
    "        observations = deque()\n",
    "        print 'game reset'\n",
    "    \n",
    "    #once we have enough data, train on a subsample of it\n",
    "    if len(replay_data) == num_to_observe:\n",
    "        #print 'fitting'\n",
    "        replay_data = random.sample(replay_data, batch_count)\n",
    "        \n",
    "        #experience replay\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for i in range(0,len(replay_data),num_frame_stacks):\n",
    "            curr_state = replay_data[i][0].get_stacked_images()\n",
    "            \n",
    "            #modified later to keep track of next state\n",
    "            next_state = np.split(curr_state, num_frame_stacks, axis=0)\n",
    "            \n",
    "            action = replay_data[i][1]\n",
    "            if action == 119:\n",
    "                action = 0\n",
    "            else:\n",
    "                action = 1\n",
    "            reward = replay_data[i][2]\n",
    "            next_frame = replay_data[i][3].gray_image\\\n",
    "                .reshape(1, input_num_rows, input_num_cols)\n",
    "            \n",
    "            #append latest state and pop the earliest\n",
    "            next_state.append(next_frame)\n",
    "            next_state.pop(0)\n",
    "            \n",
    "            is_terminal = replay_data[i][4]\n",
    "            \n",
    "                    \n",
    "            #get the highest reward possible in the next state\n",
    "            _, output = agent.predict(np.concatenate(next_state, axis=0))\n",
    "            next_reward = np.max(output)\n",
    "            \n",
    "            #if terminal, no more rewards\n",
    "            if is_terminal:\n",
    "                total_reward = -5\n",
    "            else:\n",
    "                total_reward = reward + gamma * next_reward\n",
    "\n",
    "            inputs.append(curr_state)\n",
    "            #targets.append([0,0])\n",
    "            #targets.append(output)\n",
    "            targets.append(output[0])\n",
    "            output[0][action] = total_reward\n",
    "\n",
    "            agent.fit(curr_state, output[0])\n",
    "\n",
    "        ##reset\n",
    "        replay_data = deque()\n",
    "#     reward = p.act(None)\n",
    "#     if reward != 0 and not game_over:\n",
    "#         print reward\n",
    "#         game_over = True\n",
    "#     if p.game_over():\n",
    "#         p.reset_game()\n",
    "\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._model.save('/users/momori/data/policy_agent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('/users/momori/data/policy_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<classes.StackedImages instance at 0x10878e518>,\n",
       " 119,\n",
       " 1.0,\n",
       " <classes.ImageProcessor instance at 0x1c27dafdd0>,\n",
       " False]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[1].shape\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concact_ndarray_list(ndarray, *list_values):\n",
    "    for i in range(len(list_values)):\n",
    "        print list_values[i], '\\n'\n",
    "    #np.hstack([i for i in list_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concact_ndarray_list(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sis = []\n",
    "for i in replay_data:\n",
    "    sis.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sis = [i.get_stacked_images() for i in sis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for image in sis:\n",
    "    print 'new stack'\n",
    "    for i in image:\n",
    "        io.imshow(i)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "##disable graphics for cloud computing\n",
    "os.putenv('SDL_VIDEODRIVER', 'fbcon')\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "#%%time\n",
    "output_file = 'logs/log'\n",
    "#clear log file\n",
    "open(output_file, 'w').close()\n",
    "\n",
    "#init the game flappybird\n",
    "game = FlappyBird()\n",
    "\n",
    "#set up the screen\n",
    "p = PLE(game, fps=30, display_screen=False)\n",
    "\n",
    "p.init()\n",
    "\n",
    "agent = PolicyAgent()\n",
    "observations = deque()\n",
    "\n",
    "'''values to change in AWS env'''\n",
    "max_frames = 640000 ##first frame is null\n",
    "num_frame_stacks = 4\n",
    "num_to_observe = 3200 #observations before we train the network\n",
    "\n",
    "\n",
    "batch_count = int(num_to_observe * 0.2)  #train the model by selecting some subsample of the stored replays\n",
    "                #contains lists of replay envs. each element is in the format\n",
    "                #[state, action, reward, new_state, terminal(boolean)]\n",
    "epsilon = 0.7 #choose best action 80% of the time\n",
    "gamma = 0.8 #importance of future rewards compared to current reward\n",
    "train_after_batch = True\n",
    "count_game_over = 0\n",
    "train_iteration_scores = []\n",
    "\n",
    "replay_data = deque() \n",
    "\n",
    "game_over = False\n",
    "\n",
    "#start time\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(0, max_frames):\n",
    "    if i % (max_frames/10) == 0:\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        with open(output_file, 'a') as log_handle:\n",
    "            log_handle.write('done with ' + str(i) + ' frames with ' + str(elapsed) + ' seconds\\n')\n",
    "        \n",
    "        #reset timer\n",
    "        start_time = timeit.default_timer()\n",
    "    \n",
    "    observation = p.getScreenRGB()\n",
    "    if np.max(observation) != 0:\n",
    "        observations.append(observation)\n",
    "    \n",
    "    \n",
    "    ##after num_frame_stacks frames pass, we start predicting actions\n",
    "    if len(observations) == num_frame_stacks:\n",
    "        image_processors = [ImageProcessor(observations[j])\n",
    "                           for j in range(num_frame_stacks)]\n",
    "        stacked_images = StackedImages(image_processors, num_frame_stacks)\n",
    "        if random.random() < epsilon:\n",
    "            action, output = agent.predict(stacked_images.get_stacked_images())\n",
    "        else:\n",
    "            action = np.random.choice([119, None])\n",
    "\n",
    "        ##update rewards    \n",
    "        reward = update_reward(p.act(action))\n",
    "        \n",
    "                \n",
    "        #get new state and create the replay data\n",
    "        observation = p.getScreenRGB()\n",
    "        replay_data.append([stacked_images, action, reward, \n",
    "                            ImageProcessor(observation),p.game_over()])\n",
    "        \n",
    "        #remove the oldest state\n",
    "        observations.popleft()\n",
    "        \n",
    "    else: ##first few frames, do nothing\n",
    "        p.act(None)\n",
    "    \n",
    "    #if game over, reset the game and the observations queue\n",
    "    if p.game_over():\n",
    "        count_game_over += 1\n",
    "        p.reset_game()\n",
    "        observations = deque()\n",
    "        #print 'game reset'\n",
    "        \n",
    "    #once we have enough data, train on a subsample of it\n",
    "    if len(replay_data) == num_to_observe:\n",
    "        pre_training_time = timeit.default_timer()\n",
    "         \n",
    "        agent = train_policy(agent, replay_data, num_frame_stacks=num_frame_stacks, batch_count=batch_count)\n",
    "        \n",
    "        training_time = timeit.default_timer() - pre_training_time\n",
    "        with open(output_file, 'a') as log_handle:  \n",
    "            log_handle.write('-----------TRAINING POLICY--------------\\n')\n",
    "            log_handle.write('training policy done with ' + str(training_time) + '\\n')\n",
    "            log_handle.write('--------------------------------------\\n')\n",
    "\n",
    "        \n",
    "        if train_after_batch:\n",
    "            pre_testing_time = timeit.default_timer()\n",
    "            test_score = test_game(agent)\n",
    "            train_iteration_scores.append([count_game_over, test_score]) \n",
    "            testing_time = timeit.default_timer() - pre_testing_time\n",
    "            with open(output_file, 'a') as log_handle:  \n",
    "                log_handle.write('-----------TEST PLAY--------------\\n')\n",
    "                log_handle.write('testing done with' + str(testing_time) + '\\n')\n",
    "                log_handle.write('--------------------------------------\\n')\n",
    "            \n",
    "        \n",
    "        ##reset\n",
    "        replay_data = deque()\n",
    "with open(str(train_after_batch)+'_train_iteration_scores.pkl', 'wb') as f:\n",
    "    pickle.dump(train_iteration_scores,f)\n",
    "print('finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
